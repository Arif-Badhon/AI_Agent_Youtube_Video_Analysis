Code file structure:

├── Code file structure init.txt
├── __init__.py
├── README.md
├── requirements.txt
└── src
    └── app
        ├── backend
        │   ├── agent.py
        │   ├── __init__.py
        │   ├── __pycache__
        │   │   ├── agent.cpython-312.pyc
        │   │   ├── __init__.cpython-312.pyc
        │   │   ├── utils.cpython-312.pyc
        │   │   └── visualizer.cpython-312.pyc
        │   ├── utils.py
        │   └── visualizer.py
        ├── frontend
        │   ├── gradio_app.py
        │   └── __init__.py
        ├── __init__.py
        └── __pycache__
            └── __init__.cpython-312.pyc

Here are the codes:
agent.py:
# backend/agent.py
import os
from dotenv import load_dotenv
from transformers import pipeline, AutoModelForSeq2SeqLM
import torch
import whisper
from typing import Tuple
from transformers import pipeline

# Load environment variables
load_dotenv()

#client = OpenAI(OPENAI_API_KEY=os.getenv("OPENAI_API_KEY"))

#client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

MODEL_NAME = os.getenv("MODEL_NAME")  

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=1024)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

summarizer = pipeline(
    "summarization",
    model=MODEL_NAME,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1,
    framework="pt"
    # Removed truncation and max_length here
)



summary_template = """Generate concise video summary with timestamps:
{transcript}
"""

qa_template = """Answer using video transcript:
Question: {question}
Transcript: {transcript}
"""
WHISPER_MODEL = "base"  # Change to "small", "medium", or "large" as needed

# Add this function
def get_whisper_model():
    return whisper.load_model(WHISPER_MODEL)

def truncate_text_to_1024_tokens(text, tokenizer):
    tokens = tokenizer.encode(text, truncation=False)
    if len(tokens) > 1024:
        tokens = tokens[:1024]
    return tokenizer.decode(tokens, skip_special_tokens=True)

def chunk_text(text, max_tokens=1000):
    tokens = tokenizer.encode(text, truncation=False)
    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]
    return [tokenizer.decode(chunk) for chunk in chunks]

def generate_summary(transcript, mode="medium"):
    valid_modes = ["short", "medium", "detailed"]
    mode = mode.lower()
    if mode not in valid_modes:
        mode = "medium"
    
    mode_settings = {
        "short": {"max": 150, "min": 40, "beams": 4},
        "medium": {"max": 300, "min": 120, "beams": 5},
        "detailed": {"max": 600, "min": 200, "beams": 6}
    }
    
    config = mode_settings[mode]
    truncated_transcript = truncate_text_to_1024_tokens(transcript, tokenizer)
    
    return summarizer(
        truncated_transcript,
        max_length=config["max"],
        min_length=config["min"],
        num_beams=config["beams"],
        no_repeat_ngram_size=3,
        do_sample=False,
        truncation=True
    )[0]['summary_text']


qa_pipeline = pipeline(
    "question-answering",
    model="deepset/roberta-base-squad2"
)

def answer_question(transcript, question):
    return qa_pipeline(
        question=question,
        context=transcript[:1024]  # Model's max context
    )['answer']


# agent.py - Add translation functionality
from transformers import MBart50TokenizerFast, MBartForConditionalGeneration

translation_model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
translation_tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

LANGUAGE_MAPPING = {
    "French": "fr_XX",
    "Spanish": "es_XX",
    "German": "de_DE",
    "Chinese": "zh_CN",
    "Hindi": "hi_IN",
    "Bengali": "bn_IN"
}

def translate_summary(summary, target_lang):
    translation_tokenizer.src_lang = "en_XX"
    inputs = translation_tokenizer(
        summary, 
        return_tensors="pt", 
        truncation=True, 
        max_length=1024
    )
    
    generated_tokens = translation_model.generate(
        **inputs,
        forced_bos_token_id=translation_tokenizer.lang_code_to_id[target_lang]
    )
    
    return translation_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]

from app.backend.visualizer import VisualSummarizer

visualizer = VisualSummarizer()

def generate_mindmap(transcript):
    try:
        return visualizer.create_mindmap(transcript)
    except Exception as e:
        return f"Mindmap generation failed: {str(e)}"


visualizer.py:
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
import networkx as nx
import matplotlib.pyplot as plt
import community.community_louvain as cl
from transformers import pipeline
import tempfile
import os

class VisualSummarizer:
    def __init__(self):
        self.kw_model = KeyBERT(model=SentenceTransformer("all-MiniLM-L6-v2"))
        self.ner_pipeline = pipeline("ner", aggregation_strategy="simple")

    def create_mindmap(self, text, max_nodes=15):
        keywords = self._extract_keywords(text)
        filtered_keys = self._filter_keywords(keywords, max_nodes)
        graph = self._build_graph(filtered_keys, text)
        return self._visualize_graph(graph)

    def _extract_keywords(self, text):
        kw_results = self.kw_model.extract_keywords(
            text, 
            keyphrase_ngram_range=(1, 2),
            stop_words='english',
            top_n=20
        )
        ner_results = self.ner_pipeline(text)
        combined = {kw[0]: kw[1] for kw in kw_results}
        for entity in ner_results:
            combined[entity['word']] = entity['score']
        return combined

    def _filter_keywords(self, keywords, max_nodes):
        sorted_keys = sorted(keywords.items(), key=lambda x: x[1], reverse=True)
        return [k[0] for k in sorted_keys[:max_nodes]]

    def _build_graph(self, keywords, text):
        G = nx.Graph()
        text_lower = text.lower()
        for kw in keywords:
            G.add_node(kw)
        for i, kw1 in enumerate(keywords):
            for kw2 in keywords[i+1:]:
                if f' {kw1.lower()} ' in text_lower and f' {kw2.lower()} ' in text_lower:
                    G.add_edge(kw1, kw2)
        return G

    def _visualize_graph(self, graph):
        plt.figure(figsize=(12, 8))
        if len(graph.nodes) > 0:
            partition = cl.best_partition(graph)
            pos = nx.spring_layout(graph, k=0.5)
            nx.draw_networkx_nodes(graph, pos, node_size=2500, cmap=plt.cm.RdYlBu, 
                                node_color=list(partition.values()))
            nx.draw_networkx_edges(graph, pos, alpha=0.5)
            nx.draw_networkx_labels(graph, pos, font_size=9, font_family='sans-serif')
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
        plt.savefig(temp_file.name, bbox_inches='tight')
        plt.close()
        return temp_file.name



utils.py:
# backend/utils.py (updated)
from youtube_transcript_api import YouTubeTranscriptApi
from langdetect import detect, DetectorFactory
import re
from pytube import YouTube
import whisper
import tempfile
import os

# For consistent results
DetectorFactory.seed = 0

def extract_video_id(url):
    regex = r"(?:v=|\/)([0-9A-Za-z_-]{11}).*"
    match = re.search(regex, url)
    return match.group(1) if match else None

def get_transcript_with_lang(url):
    video_id = extract_video_id(url)
    try:
        # Get available transcripts
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        
        # Try to find manual transcript first
        try:
            transcript = transcript_list.find_manually_created_transcript()
        except:
            # Fallback to auto-generated transcript
            transcript = transcript_list.find_generated_transcript([transcript.language_code for transcript in transcript_list])
        
        # Get text and language
        transcript_text = " ".join([entry['text'] for entry in transcript.fetch()])
        return transcript_text, transcript.language_code
    
    except Exception as e:
        # Fallback to language detection from text
        transcript = YouTubeTranscriptApi.get_transcript(video_id)
        transcript_text = " ".join([entry['text'] for entry in transcript])
        return transcript_text, detect(transcript_text)

def get_transcript(url):
    try:
        # First try YouTube captions
        video_id = extract_video_id(url)
        transcript = YouTubeTranscriptApi.get_transcript(video_id)
        return " ".join([entry['text'] for entry in transcript])
    except:
        # Fallback to audio transcription
        try:
            audio_path = download_audio(url)
            return transcribe_audio(audio_path)
        except Exception as e:
            raise ValueError(f"Both methods failed: {str(e)}")

def download_audio(url):
    try:
        yt = YouTube(url)
        audio_stream = yt.streams.filter(only_audio=True).first()
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
        audio_stream.download(filename=temp_file.name)
        return temp_file.name
    except Exception as e:
        raise ValueError(f"Audio download failed: {str(e)}")

def transcribe_audio(audio_path):
    try:
        model = whisper.load_model("base")  # Use 'small' or 'medium' for better accuracy
        result = model.transcribe(audio_path)
        return result["text"]
    except Exception as e:
        raise ValueError(f"Transcription failed: {str(e)}")
    finally:
        if os.path.exists(audio_path):
            os.remove(audio_path)

gradio_app.py:
import gradio as gr
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from app.backend.agent import generate_summary, answer_question, translate_summary, generate_mindmap
from app.backend.utils import *

custom_theme = gr.themes.Default(
    primary_hue="blue",
    secondary_hue="slate",
    neutral_hue="gray"
).set(
    body_background_fill="white",
    block_background_fill="white",
    # Changed text color parameter:
    body_text_color="black",
    button_primary_background_fill="#000000",
    button_primary_text_color="white",
    button_primary_background_fill_hover="#1a1a1a",
    block_label_background_fill="#000000",
    block_label_text_color="white"
)


css = """
.gradio-container {max-width: 900px!important}
footer {visibility: hidden}
@import url('https://fonts.boomla.com/bangla.css');
body {font-family: 'SolaimanLipi', sans-serif!important}
.dark .summary-box textarea {background-color: #1a1a1a!important}
.md_output {margin-top: 10px!important}
"""

LANGUAGE_MAPPING = {
    "English": "en_XX",
    "French": "fr_XX",
    "Spanish": "es_XX", 
    "German": "de_DE",
    "Chinese": "zh_CN",
    "Hindi": "hi_IN",
    "Arabic": "ar_AR",
    "Russian": "ru_RU",
    "Bengali": "bn_IN"
}

with gr.Blocks(theme=custom_theme, css=css) as app:
    gr.Markdown("# YouTube AI Analyzer 3.0 🚀")
    
    with gr.Tab("Video Analysis"):
        with gr.Row():
            url_input = gr.Textbox(label="YouTube URL", placeholder="Paste video URL here...")
            summary_output = gr.Textbox(label="Video Summary", lines=5, interactive=False)
        
        with gr.Row():
            with gr.Column(scale=2):
                mode_dropdown = gr.Dropdown(
                    choices=["Short", "Medium", "Detailed"],
                    value="Medium",
                    label="Summary Length",
                    interactive=True
                )
        with gr.Row():
            analyze_btn = gr.Button("Analyze Video", variant="primary")
            vis_output = gr.Image(label="Visual Summary (Mind Map)", show_label=True)
            
        with gr.Accordion("Translation Settings", open=True):
            with gr.Row():
                lang_dropdown = gr.Dropdown(
                    choices=list(LANGUAGE_MAPPING.keys()),
                    value="English",
                    label="Target Language",
                    interactive=True
                )
                translate_btn = gr.Button("Translate Summary", variant="secondary")
            
            translated_output = gr.Textbox(label="Translated Summary", lines=5, interactive=False)

    def analyze_video_handler(url, mode):
        if not url.startswith("https://www.youtube.com/"):
            raise gr.Error("Invalid YouTube URL format")
        
        try:
            transcript = get_transcript(url)
            summary = generate_summary(transcript, mode.lower())
            mindmap_path = generate_mindmap(summary)
            return summary, mindmap_path  # Return both values
        except Exception as e:
            return f"Analysis failed: {str(e)}", None  # Return None for image


    def translate_handler(summary, target_lang):
        if not summary or summary.startswith("Analysis failed"):
            raise gr.Error("Generate a valid summary first")
        
        lang_code = LANGUAGE_MAPPING.get(target_lang, "en_XX")
        try:
            return translate_summary(summary, lang_code)
        except Exception as e:
            return f"Translation failed: {str(e)}"

    # Event bindings
    analyze_btn.click(
        fn=analyze_video_handler,
        inputs=[url_input, mode_dropdown],
        outputs=[summary_output, vis_output],
        show_progress="full"
    )
    
    translate_btn.click(
        fn=translate_handler,
        inputs=[summary_output, lang_dropdown],
        outputs=translated_output
    )

if __name__ == "__main__":
    app.launch()


requirements.txt:
# Keep only these core dependencies
youtube-transcript-api
setuptools>=68.0.0
gradio>=4.25.0
transformers>=4.37.0
torch>=2.2.2
accelerate>=0.31.0
sentencepiece
protobuf
langdetect==1.0.9
pytube==15.0.0
openai-whisper==20231117
ffmpeg-python==0.2.0
keybert==0.7.0
sentence-transformers==2.7.0
networkx==3.2.1
matplotlib==3.8.2
python-louvain==0.16
python-dotenv


.gitignore: 
.env
venv/
__pycache__/
*.pyc
.venv




