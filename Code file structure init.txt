Code structure:

.
├── assests
│   └── hermes.png
├── Code file structure init.txt
├── __init__.py
├── mindmap.png
├── README.md
├── requirements.txt
├── run.sh
├── run_test.sh
└── src
    └── app
        ├── backend
        │   ├── agent.py
        │   ├── __init__.py
        │   ├── __pycache__
        │   │   ├── agent.cpython-312.pyc
        │   │   ├── __init__.cpython-312.pyc
        │   │   ├── utils.cpython-312.pyc
        │   │   └── visualizer.cpython-312.pyc
        │   ├── utils.py
        │   └── visualizer.py
        ├── frontend
        │   ├── gradio_app.py
        │   ├── __init__.py
        │   └── __pycache__
        │       ├── gradio_app.cpython-312.pyc
        │       └── __init__.cpython-312.pyc
        ├── __init__.py
        └── __pycache__
            └── __init__.cpython-312.pyc


File src > app > backend > __init__.py:


File src > app > backend > agent.py:
from transformers import (
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    MBart50TokenizerFast,
    MBartForConditionalGeneration,
    pipeline,
    AutoModelForSeq2SeqLM,
)
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import torch


class QAAgent:
    def __init__(self):
        # QA System
        self.embedder = SentenceTransformer("all-MiniLM-L6-v2")
        qa_model = AutoModelForQuestionAnswering.from_pretrained(
            "deepset/roberta-large-squad2"
        )
        qa_tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-large-squad2")
        self.qa_model = pipeline(
            "question-answering",
            model=qa_model,
            tokenizer=qa_tokenizer,
            max_seq_len=512,
            device=0 if torch.cuda.is_available() else -1,
        )
        self.cache = {}

        # Translation System
        self.translator = MBartForConditionalGeneration.from_pretrained(
            "facebook/mbart-large-50-many-to-many-mmt"
        )
        self.trans_tokenizer = MBart50TokenizerFast.from_pretrained(
            "facebook/mbart-large-50-many-to-many-mmt"
        )
        self.lang_map = {
            "English": "en_XX",
            "Bengali": "bn_IN",
            "Hindi": "hi_IN",
            "Chinese": "zh_CN",
            "Spanish": "es_XX",
            "French": "fr_XX",
            "Arabic": "ar_AR",
            "Russian": "ru_RU",
        }
        self.question_generator = pipeline(
            "text2text-generation",
            model="mrm8488/t5-base-finetuned-question-generation-ap",
            device=0 if torch.cuda.is_available() else -1,
            torch_dtype=torch.float32
        )

    # app/backend/agent.py
    def generate_questions(self, text, num_questions=3):
        cleaned_text = ' '.join(text.split()[:500])  # Limit input size
        prompt = f"generate questions: {cleaned_text}"
        try:
            results = self.question_generator(
                prompt,
                max_length=80,
                num_beams=5,
                num_return_sequences=num_questions,
                early_stopping=True
            )
            return [q['generated_text'].strip().replace('question: ', '') 
                    for q in results if '?' in q['generated_text']]
        except Exception as e:
            print(f"Question generation failed: {e}")
            return []


    def chunk_text(self, text, chunk_size=300):
        """Enhanced chunking with overlap"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0

        for i in range(len(words)):
            current_chunk.append(words[i])
            current_length += 1

            if current_length >= chunk_size:
                chunks.append(" ".join(current_chunk))
                # Keep 20% overlap
                current_chunk = current_chunk[-int(chunk_size * 0.2) :]
                current_length = len(current_chunk)

        if current_chunk:
            chunks.append(" ".join(current_chunk))
        return chunks

    def process_transcript(self, url, transcript):
        video_id = url.split("v=")[-1]
        if video_id not in self.cache:
            chunks = self.chunk_text(transcript)
            self.cache[video_id] = {
                "chunks": chunks,
                "embeddings": self.embedder.encode(chunks),
            }

    def answer_question(self, url, question):
        video_id = url.split("v=")[-1]
        if video_id not in self.cache:
            raise ValueError("Process transcript first")

        data = self.cache[video_id]
        question_embed = self.embedder.encode([question])

        similarities = cosine_similarity(question_embed, data["embeddings"])[0]

        # Handle case with few chunks
        valid_k = min(5, len(similarities))
        if valid_k == 0:
            return "❌ Not enough context to answer this question"

        top_idxs = np.argsort(similarities)[
            -valid_k:
        ]  # Use argsort instead of argpartition
        context = " ".join([data["chunks"][i] for i in top_idxs][-3:])

        if len(context.split()) > 500:
            context = " ".join(context.split()[:500])

        result = self.qa_model(
            question=question,
            context=context,
            max_answer_len=150,
            handle_impossible_answer=True,
        )

        if (
            result["answer"].strip() == "" or result["score"] < 0.12
        ):  # Slightly lower threshold
            return f"I'm unsure, but here's a relevant excerpt: {context[:200]}..."
        return f"{result['answer']} (Confidence: {result['score']:.0%})"

    def translate_text(self, text, target_lang):
        if target_lang not in self.lang_map:
            raise ValueError(f"Unsupported language: {target_lang}")

        self.trans_tokenizer.src_lang = "en_XX"
        encoded = self.trans_tokenizer(text, return_tensors="pt")
        generated_tokens = self.translator.generate(
            **encoded,
            forced_bos_token_id=self.trans_tokenizer.lang_code_to_id[
                self.lang_map[target_lang]
            ],
        )
        return self.trans_tokenizer.batch_decode(
            generated_tokens, skip_special_tokens=True
        )[0]


# Explicitly load tokenizer with model_max_length
summarizer_tokenizer = AutoTokenizer.from_pretrained(
    "facebook/bart-large-cnn"
    # model_max_length=1024  # BART's actual max input length
)
summarizer_model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn")

summarizer = pipeline(
    "summarization",
    model=summarizer_model,
    tokenizer=summarizer_tokenizer,
    device=0 if torch.cuda.is_available() else -1,
)


def chunk_text(text, max_tokens=900):
    """Split text by tokens instead of words"""
    tokenizer = summarizer_tokenizer  # Reuse the initialized tokenizer
    tokens = tokenizer.encode(text, truncation=False, add_special_tokens=False)

    for i in range(0, len(tokens), max_tokens):
        chunk = tokens[i : i + max_tokens]
        yield tokenizer.decode(chunk, skip_special_tokens=True)


def generate_summary(transcript, mode):
    # Set summary lengths based on mode
    if mode.lower() == "short":
        max_length, min_length = 40, 20
    elif mode.lower() == "medium":
        max_length, min_length = 100, 40
    else:  # detailed
        max_length, min_length = 150, 60

    # Always chunk transcript if too long
    summaries = []
    for chunk in chunk_text(transcript, max_tokens=900):
        # Explicitly truncate input to 1024 tokens
        inputs = summarizer_tokenizer(
            chunk, max_length=1024, truncation=True, return_tensors="pt"
        )
        truncated_chunk = summarizer_tokenizer.decode(inputs["input_ids"][0])

        summary = summarizer(
            truncated_chunk,  # Use truncated input
            max_length=max_length,
            min_length=min_length,
            do_sample=False,
        )[0]["summary_text"]
        summaries.append(summary)

    # If multiple chunks, summarize the summaries
    if len(summaries) > 1:
        combined = " ".join(summaries)
        final_summary = summarizer(
            combined,
            max_length=max_length,
            min_length=min_length,
            do_sample=False,
            truncation=True,
        )[0]["summary_text"]
        return final_summary
    else:
        return summaries[0]


def generate_mindmap(summary):
    """Generate mindmap visualization"""
    words = [word for word in summary.split() if len(word) > 3][:15]
    G = nx.Graph()
    for i, word in enumerate(words):
        G.add_node(word)
        if i > 0:
            G.add_edge(words[i - 1], word)

    plt.figure(figsize=(16, 10))
    pos = nx.spring_layout(G, k=0.5)
    nx.draw(
        G,
        pos,
        with_labels=True,
        node_size=2500,
        node_color="skyblue",
        font_size=10,
        font_weight="bold",
        edge_color="gray",
    )
    plt.savefig("mindmap.png", bbox_inches="tight")
    plt.close()
    return "mindmap.png"


qa_agent = QAAgent()


File src > app > backend > utils.py:
import requests
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound

def get_video_id(url):
    # Handles both youtu.be and youtube.com URLs
    if "youtu.be/" in url:
        return url.split("youtu.be/")[-1].split("?")[0]
    elif "youtube.com/watch?v=" in url:
        return url.split("v=")[-1].split("&")[0]
    return None

def get_transcript(url):
    try:
        video_id = get_video_id(url)
        if not video_id:
            raise ValueError("Invalid YouTube URL")
        
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        transcript = transcript_list.find_generated_transcript(['en'])
        
        # Access text via object attribute instead of dictionary key
        return " ".join([snippet.text for snippet in transcript.fetch()])
        
    except (TranscriptsDisabled, NoTranscriptFound) as e:
        raise RuntimeError(f"Transcript unavailable: {str(e)}")
    except Exception as e:
        raise RuntimeError(f"Transcript error: {str(e)}")

def get_video_metadata(url):
    try:
        response = requests.get(
            f"https://www.youtube.com/oembed?url={url}&format=json"
        )
        if response.status_code == 200:
            data = response.json()
            return {
                "title": data.get("title", "No title"),
                "channel": data.get("author_name", "Unknown channel"),
                "thumbnail_url": data.get("thumbnail_url", "")
            }
        return None
    except Exception as e:
        print(f"Metadata error: {e}")
        return None


File src > app > backend > visualizer.py:
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
import networkx as nx
import matplotlib.pyplot as plt
import community.community_louvain as cl
from transformers import pipeline
import tempfile


class VisualSummarizer:
    def __init__(self):
        self.kw_model = KeyBERT(model=SentenceTransformer("all-MiniLM-L6-v2"))
        self.ner_pipeline = pipeline(
            "ner", model="dslim/bert-base-NER", aggregation_strategy="simple"
        )

    def create_mindmap(self, text, max_nodes=15):
        keywords = self._extract_keywords(text)
        filtered_keys = self._filter_keywords(keywords, max_nodes)
        graph = self._build_graph(filtered_keys, text)
        return self._visualize_graph(graph)

    def _extract_keywords(self, text):
        kw_results = self.kw_model.extract_keywords(
            text, keyphrase_ngram_range=(1, 2), stop_words="english", top_n=20
        )
        ner_results = self.ner_pipeline(text)
        combined = {kw[0]: kw[1] for kw in kw_results}
        for entity in ner_results:
            combined[entity["word"]] = entity["score"]
        return combined

    def _filter_keywords(self, keywords, max_nodes):
        sorted_keys = sorted(keywords.items(), key=lambda x: x[1], reverse=True)
        return [k[0] for k in sorted_keys[:max_nodes]]

    def _build_graph(self, keywords, text):
        G = nx.Graph()
        text_lower = text.lower()
        for kw in keywords:
            G.add_node(kw)
        for i, kw1 in enumerate(keywords):
            for kw2 in keywords[i + 1 :]:
                if (
                    f" {kw1.lower()} " in text_lower
                    and f" {kw2.lower()} " in text_lower
                ):
                    G.add_edge(kw1, kw2)
        return G

    def _visualize_graph(self, graph):
        plt.figure(figsize=(12, 8))
        if len(graph.nodes) > 0:
            partition = cl.best_partition(graph)
            pos = nx.spring_layout(graph, k=0.5)
            nx.draw_networkx_nodes(
                graph,
                pos,
                node_size=2500,
                cmap=plt.cm.RdYlBu,
                node_color=list(partition.values()),
            )
            nx.draw_networkx_edges(graph, pos, alpha=0.5)
            nx.draw_networkx_labels(graph, pos, font_size=9, font_family="sans-serif")
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
        plt.savefig(temp_file.name, bbox_inches="tight")
        plt.close()
        return temp_file.name


File src > app > frontend > __init__.py:

File src > app > frontend > gradio_app.py:
import gradio as gr
import sys
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from app.backend.agent import generate_summary, generate_mindmap, qa_agent
from app.backend.utils import get_transcript, get_video_metadata

import warnings
from transformers import logging
logging.set_verbosity_error()
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Futuristic dark theme with neon accents
custom_theme = gr.themes.Default(
    primary_hue="blue", 
    secondary_hue="slate",
    neutral_hue="slate"
).set(
    button_primary_background_fill="#00D4FF",
    button_primary_text_color="#000",
    button_primary_background_fill_hover="#0099CC",
    button_secondary_background_fill="rgba(0, 212, 255, 0.1)",
    button_secondary_text_color="#00D4FF",
    button_secondary_background_fill_hover="rgba(0, 212, 255, 0.2)",
    block_background_fill="#0A0A0F",
    background_fill_primary="#050508",
    background_fill_secondary="#0F0F15",
)

css = """
@import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700;900&family=Exo+2:wght@300;400;600&display=swap');

.gradio-container {
    max-width: 100vw !important; 
    margin: 0 !important;
    background: linear-gradient(135deg, #050508 0%, #0A0A0F 50%, #0F0F15 100%) !important;
    min-height: 100vh;
}

footer {visibility: hidden}

/* Neon title styling */
.neon-title {
    font-family: 'Orbitron', monospace !important;
    font-size: 4rem !important;
    font-weight: 900 !important;
    text-align: center !important;
    color: #00D4FF !important;
    text-shadow: 
        0 0 5px #00D4FF,
        0 0 10px #00D4FF,
        0 0 20px #00D4FF,
        0 0 40px #00D4FF,
        0 0 80px #00D4FF !important;
    letter-spacing: 0.1em !important;
    margin: 2rem 0 !important;
    animation: pulse-glow 2s ease-in-out infinite alternate;
}

@keyframes pulse-glow {
    from {
        text-shadow: 
            0 0 5px #00D4FF,
            0 0 10px #00D4FF,
            0 0 20px #00D4FF,
            0 0 40px #00D4FF;
    }
    to {
        text-shadow: 
            0 0 10px #00D4FF,
            0 0 20px #00D4FF,
            0 0 30px #00D4FF,
            0 0 60px #00D4FF,
            0 0 100px #00D4FF;
    }
}

/* Main layout grid */
.main-grid {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 2rem;
    max-width: 1400px;
    margin: 0 auto;
    padding: 2rem;
}

/* Neon card styling */
.neon-card {
    background: rgba(15, 15, 21, 0.8) !important;
    border: 2px solid rgba(0, 212, 255, 0.3) !important;
    border-radius: 20px !important;
    padding: 2rem !important;
    box-shadow: 
        0 0 20px rgba(0, 212, 255, 0.1),
        inset 0 0 20px rgba(0, 212, 255, 0.05) !important;
    backdrop-filter: blur(10px) !important;
    transition: all 0.3s ease !important;
}

.neon-card:hover {
    border-color: rgba(0, 212, 255, 0.6) !important;
    box-shadow: 
        0 0 30px rgba(0, 212, 255, 0.2),
        inset 0 0 30px rgba(0, 212, 255, 0.1) !important;
    transform: translateY(-5px) !important;
}

/* Input section styling */
.input-section {
    grid-column: 1 / -1;
    margin-bottom: 2rem;
}

.neon-input {
    background: rgba(0, 0, 0, 0.5) !important;
    border: 2px solid rgba(0, 212, 255, 0.3) !important;
    border-radius: 15px !important;
    color: #00D4FF !important;
    font-family: 'Exo 2', sans-serif !important;
    padding: 1rem !important;
}

.neon-input:focus {
    border-color: #00D4FF !important;
    box-shadow: 0 0 20px rgba(0, 212, 255, 0.3) !important;
}

/* Video preview styling */
.video-preview {
    position: relative;
    overflow: hidden;
}

.video-preview img {
    border-radius: 15px !important;
    width: 100% !important;
    height: auto !important;
    border: 2px solid rgba(0, 212, 255, 0.3) !important;
}

/* Summary section styling */
.summary-section {
    font-family: 'Exo 2', sans-serif !important;
}

#summary_output, #translated_output {
    background: rgba(0, 0, 0, 0.6) !important;
    color: #E0E0FF !important;
    padding: 1.5rem !important;
    border-radius: 15px !important;
    border: 1px solid rgba(0, 212, 255, 0.2) !important;
    font-family: 'Exo 2', sans-serif !important;
    line-height: 1.6 !important;
}

/* Mind map styling */
.mindmap-container {
    grid-column: 1 / -1;
    margin-top: 2rem;
}

#mindmap-output img {
    border-radius: 15px !important;
    border: 2px solid rgba(0, 212, 255, 0.3) !important;
    background: rgba(0, 0, 0, 0.3) !important;
    width: 100% !important;
    height: auto !important;
}

/* Q&A section styling */
.qa-section {
    grid-column: 1 / -1;
    margin-top: 2rem;
}

.chatbot {
    background: rgba(0, 0, 0, 0.4) !important;
    border: 2px solid rgba(0, 212, 255, 0.3) !important;
    border-radius: 20px !important;
    min-height: 400px !important;
}

/* Button styling */
.neon-button {
    background: linear-gradient(45deg, #00D4FF, #0099CC) !important;
    color: #000 !important;
    border: none !important;
    border-radius: 25px !important;
    padding: 12px 24px !important;
    font-family: 'Orbitron', monospace !important;
    font-weight: 600 !important;
    text-transform: uppercase !important;
    letter-spacing: 1px !important;
    box-shadow: 0 0 20px rgba(0, 212, 255, 0.3) !important;
    transition: all 0.3s ease !important;
}

.neon-button:hover {
    background: linear-gradient(45deg, #33E5FF, #00BBEE) !important;
    box-shadow: 0 0 30px rgba(0, 212, 255, 0.5) !important;
    transform: scale(1.05) !important;
}

/* Suggested questions styling */
.suggested-question {
    background: rgba(0, 212, 255, 0.1) !important;
    border: 1px solid rgba(0, 212, 255, 0.3) !important;
    color: #00D4FF !important;
    border-radius: 15px !important;
    padding: 10px 15px !important;
    margin: 5px !important;
    font-family: 'Exo 2', sans-serif !important;
    transition: all 0.3s ease !important;
}

.suggested-question:hover {
    background: rgba(0, 212, 255, 0.2) !important;
    border-color: #00D4FF !important;
    box-shadow: 0 0 15px rgba(0, 212, 255, 0.3) !important;
}

/* Accordion styling */
.accordion {
    background: rgba(15, 15, 21, 0.6) !important;
    border: 1px solid rgba(0, 212, 255, 0.2) !important;
    border-radius: 15px !important;
    margin: 1rem 0 !important;
}

/* Copy button styling */
.copy-btn {
    background: rgba(0, 212, 255, 0.2) !important;
    color: #00D4FF !important;
    border: 1px solid rgba(0, 212, 255, 0.4) !important;
    border-radius: 10px !important;
    font-family: 'Exo 2', sans-serif !important;
}

.copy-btn:hover {
    background: rgba(0, 212, 255, 0.3) !important;
    box-shadow: 0 0 15px rgba(0, 212, 255, 0.3) !important;
}

/* Section headers */
.section-header {
    font-family: 'Orbitron', monospace !important;
    color: #00D4FF !important;
    font-size: 1.5rem !important;
    font-weight: 700 !important;
    margin-bottom: 1rem !important;
    text-transform: uppercase !important;
    letter-spacing: 2px !important;
}

/* Responsive design */
@media (max-width: 768px) {
    .main-grid {
        grid-template-columns: 1fr;
        padding: 1rem;
    }
    
    .neon-title {
        font-size: 2.5rem !important;
    }
}

/* Custom scrollbar */
::-webkit-scrollbar {
    width: 8px;
}

::-webkit-scrollbar-track {
    background: rgba(0, 0, 0, 0.3);
}

::-webkit-scrollbar-thumb {
    background: linear-gradient(45deg, #00D4FF, #0099CC);
    border-radius: 10px;
}

::-webkit-scrollbar-thumb:hover {
    background: linear-gradient(45deg, #33E5FF, #00BBEE);
}
"""

def analyze_video(url, mode):
    try:
        transcript = get_transcript(url)
        qa_agent.process_transcript(url, transcript)
        if not transcript:
            raise ValueError("Transcript is empty")
        summary = generate_summary(transcript, mode)
        mindmap = generate_mindmap(summary)
        return (f"### {mode.capitalize()} Summary\n\n{summary}", mindmap, "")
    except Exception as e:
        return f"**Error:** {str(e)}", None, ""

def handle_translation(summary, lang):
    try:
        translated = qa_agent.translate_text(summary, lang)
        return f"### {lang} Translation\n\n{translated}"
    except Exception as e:
        return f"**Translation Error:** {str(e)}"

def handle_qa(history, url, question):
    try:
        answer = qa_agent.answer_question(url, question)
        return history + [
            {"role": "user", "content": question},
            {"role": "assistant", "content": answer},
        ]
    except Exception as e:
        return history + [
            {"role": "user", "content": question},
            {"role": "assistant", "content": f"⚠️ Error: {str(e)}"},
        ]

def update_suggested_questions(summary):
    """Generate questions and return visibility state"""
    if not summary or "Error:" in summary:
        return [gr.update(visible=False)]*3 + [gr.Row.update(visible=False)]
    
    try:
        questions = qa_agent.generate_questions(summary, num_questions=3)
        return [
            gr.update(visible=True, value=questions[0]),
            gr.update(visible=len(questions)>1, value=questions[1] if len(questions)>1 else ""),
            gr.update(visible=len(questions)>2, value=questions[2] if len(questions)>2 else ""),
            gr.Row.update(visible=len(questions)>0)
        ]
    except Exception as e:
        print(f"Question generation error: {e}")
        return [gr.update(visible=False)]*3 + [gr.Row.update(visible=False)]

def update_preview(url):
    if not url or not url.startswith(("https://www.youtube.com", "https://youtu.be")):
        return [None, "", "", False]
    
    metadata = get_video_metadata(url)
    if not metadata:
        return [
            None,
            "### Video Not Found",
            "The video may be private or unavailable",
            True
        ]
    
    return [
        metadata["thumbnail_url"],
        f"### {metadata['title']}",
        f"**Channel**: {metadata['channel']}",
        True
    ]

def update_row_visibility(should_show):
    return gr.Row.update(visible=should_show)

def use_suggested_question(question):
    """Insert question into input and clear suggestions"""
    return (
        question,
        *[gr.update(visible=False) for _ in range(3)],
        gr.Row.update(visible=False)
    )

with gr.Blocks(theme=custom_theme, css=css, title="Hermes AI") as app:
    
    # ========== Neon Header ==========
    gr.HTML('<h1 class="neon-title">HERMES AI</h1>')
    
    # ========== Input Section ==========
    with gr.Row(elem_classes="input-section"):
        with gr.Column(elem_classes="neon-card"):
            gr.HTML('<h2 class="section-header">Video Analysis</h2>')
            with gr.Row():
                url_input = gr.Textbox(
                    label="YouTube URL",
                    placeholder="https://youtube.com/watch?v=...",
                    max_lines=1,
                    scale=3,
                    elem_classes="neon-input"
                )
                mode_selector = gr.Radio(
                    choices=["Short", "Medium", "Detailed"],
                    value="Medium",
                    label="Summary Mode",
                    scale=1,
                )
            analyze_btn = gr.Button(
                "🚀 Analyze Video", 
                variant="primary", 
                elem_classes="neon-button",
                size="lg"
            )
    
    # ========== Video Preview (Hidden by default) ==========
    with gr.Row(visible=False, elem_classes="neon-card") as preview_row:
        with gr.Column(scale=1, elem_classes="video-preview"):
            thumbnail = gr.Image(label="Video Preview", show_label=False)
        with gr.Column(scale=2):
            video_title = gr.Markdown()
            video_channel = gr.Markdown()
    
    visibility_tracker = gr.Textbox(visible=False)
    
    # ========== Main Content Grid ==========
    with gr.Row():
        # Left Column - Video & Mind Map
        with gr.Column(scale=1):
            # Summary Section
            with gr.Column(elem_classes="neon-card summary-section"):
                gr.HTML('<h2 class="section-header">Summary</h2>')
                summary_output = gr.Markdown(elem_id="summary_output")
                copy_summary_btn = gr.Button("📋 Copy Summary", elem_classes="copy-btn")
        
        # Right Column - Translation & Q&A
        with gr.Column(scale=1):
            # Translation Section
            with gr.Column(elem_classes="neon-card"):
                gr.HTML('<h2 class="section-header">🌍 Translation</h2>')
                with gr.Row():
                    lang_selector = gr.Dropdown(
                        choices=[
                            "English", "Bengali", "Hindi", "Chinese", 
                            "Spanish", "French", "Arabic", "Russian"
                        ],
                        value="English",
                        label="Target Language",
                        scale=2
                    )
                    translate_btn = gr.Button("Translate", elem_classes="neon-button", scale=1)
                translated_output = gr.Markdown(elem_id="translated_output")
                copy_translate_btn = gr.Button("📋 Copy Translation", elem_classes="copy-btn")
    
    # ========== Mind Map Section ==========
    with gr.Row(elem_classes="mindmap-container"):
        with gr.Column(elem_classes="neon-card"):
            gr.HTML('<h2 class="section-header">🧠 Interactive Mind Map</h2>')
            mindmap_output = gr.Image(
                label="Concept Visualization",
                elem_id="mindmap-output",
                show_download_button=True,
                show_label=False
            )
    
    # ========== Q&A Section ==========
    with gr.Row(elem_classes="qa-section"):
        with gr.Column(elem_classes="neon-card"):
            gr.HTML('<h2 class="section-header">💬 AI-Powered Q&A</h2>')
            qa_chat = gr.Chatbot(
                height=400,
                avatar_images=(
                    "https://i.imgur.com/7kQEsHU.png",
                    "https://i.imgur.com/8EeSUQ3.png",
                ),
                show_label=False,
                type="messages",
                elem_classes="chatbot"
            )
            
            # Suggested Questions
            with gr.Row(visible=False) as suggested_questions_row:
                suggested_btns = [
                    gr.Button(visible=False, elem_classes="suggested-question") 
                    for _ in range(3)
                ]
            
            # Input Section
            with gr.Row():
                qa_input = gr.Textbox(
                    placeholder="Ask anything about the video...",
                    show_label=False,
                    scale=4,
                    elem_classes="neon-input"
                )
                qa_btn = gr.Button("Ask AI", elem_classes="neon-button", scale=1)

    # ========== Event Handling ==========
    url_input.input(
        fn=update_preview,
        inputs=url_input,
        outputs=[thumbnail, video_title, video_channel, visibility_tracker]
    )

    visibility_tracker.change(
        fn=update_row_visibility,
        inputs=visibility_tracker,
        outputs=preview_row
    )

    analyze_btn.click(
        analyze_video,
        inputs=[url_input, mode_selector],
        outputs=[summary_output, mindmap_output, translated_output]
    ).then(
        update_suggested_questions,
        inputs=summary_output,
        outputs=[*suggested_btns, suggested_questions_row]
    )

    for btn in suggested_btns:
        btn.click(
            fn=use_suggested_question,
            inputs=btn,
            outputs=[qa_input, *suggested_btns, suggested_questions_row]
        ).then(
            fn=handle_qa,
            inputs=[qa_chat, url_input, qa_input],
            outputs=qa_chat
        )

    translate_btn.click(
        handle_translation,
        inputs=[summary_output, lang_selector],
        outputs=translated_output,
    )

    qa_btn.click(handle_qa, inputs=[qa_chat, url_input, qa_input], outputs=qa_chat)
    qa_input.submit(handle_qa, inputs=[qa_chat, url_input, qa_input], outputs=qa_chat)

    # Copy functionality
    copy_summary_btn.click(
        None,
        inputs=summary_output,
        outputs=None,
        js="async (text) => { await navigator.clipboard.writeText(text); }",
    )
    copy_translate_btn.click(
        None,
        inputs=translated_output,
        outputs=None,
        js="async (text) => { await navigator.clipboard.writeText(text); }",
    )

if __name__ == "__main__":
    app.launch(share=False, server_name="0.0.0.0")


File src > app > __init__.py:

File __init__.py:

File .env:
OPENAI_API_KEY=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
#MODEL_NAME = "sshleifer/distilbart-cnn-12-6"
MODEL_NAME = "facebook/bart-large-cnn"
ALLOWED_DOMAINS=www.youtube.com,youtu.be

File requirements.txt:
gradio>=4.0.0
youtube-transcript-api>=0.6.1
transformers>=4.30.0
sentence-transformers>=2.7.0
scikit-learn>=1.3.2
torch>=2.0.0
numpy>=1.24.3
networkx>=3.1
matplotlib>=3.7.0
keybert>=0.7.0
python-louvain>=0.16
requests>=2.26.0
sentencepiece