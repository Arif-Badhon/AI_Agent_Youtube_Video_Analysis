Code file structure:

.
├── Code file structure init.txt
├── __init__.py
├── README.md
├── requirements.txt
└── src
    └── app
        ├── backend
        │   ├── agent.py
        │   ├── __init__.py
        │   ├── __pycache__
        │   │   ├── agent.cpython-312.pyc
        │   │   ├── __init__.cpython-312.pyc
        │   │   ├── utils.cpython-312.pyc
        │   │   └── visualizer.cpython-312.pyc
        │   ├── utils.py
        │   └── visualizer.py
        ├── frontend
        │   ├── gradio_app.py
        │   └── __init__.py
        ├── __init__.py
        └── __pycache__
            └── __init__.cpython-312.pyc


Here are the codes:

File src > app > backend > __init__.py:

File src > app > backend > agent.py:
# backend/agent.py
import os
from dotenv import load_dotenv
from transformers import pipeline, AutoModelForSeq2SeqLM
import torch
import whisper
from typing import Tuple
from transformers import pipeline

# Load environment variables
load_dotenv()

#client = OpenAI(OPENAI_API_KEY=os.getenv("OPENAI_API_KEY"))

#client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

MODEL_NAME = os.getenv("MODEL_NAME")  

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=1024)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

summarizer = pipeline(
    "summarization",
    model=MODEL_NAME,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1,
    framework="pt"
    # Removed truncation and max_length here
)



summary_template = """Generate concise video summary with timestamps:
{transcript}
"""

qa_template = """Answer using video transcript:
Question: {question}
Transcript: {transcript}
"""
WHISPER_MODEL = "base"  # Change to "small", "medium", or "large" as needed

# Add this function
def get_whisper_model():
    return whisper.load_model(WHISPER_MODEL)

def truncate_text_to_1024_tokens(text, tokenizer):
    tokens = tokenizer.encode(text, truncation=False)
    if len(tokens) > 1024:
        tokens = tokens[:1024]
    return tokenizer.decode(tokens, skip_special_tokens=True)

def chunk_text(text, max_tokens=1000):
    tokens = tokenizer.encode(text, truncation=False)
    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]
    return [tokenizer.decode(chunk) for chunk in chunks]

def generate_summary(transcript, mode="medium"):
    valid_modes = ["short", "medium", "detailed"]
    mode = mode.lower()
    if mode not in valid_modes:
        mode = "medium"
    
    mode_settings = {
        "short": {"max": 150, "min": 40, "beams": 4},
        "medium": {"max": 300, "min": 120, "beams": 5},
        "detailed": {"max": 600, "min": 200, "beams": 6}
    }
    
    config = mode_settings[mode]
    truncated_transcript = truncate_text_to_1024_tokens(transcript, tokenizer)
    
    return summarizer(
        truncated_transcript,
        max_length=config["max"],
        min_length=config["min"],
        num_beams=config["beams"],
        no_repeat_ngram_size=3,
        do_sample=False,
        truncation=True
    )[0]['summary_text']




qa_pipeline = pipeline(
    "question-answering",
    model="deepset/roberta-base-squad2"
)

def answer_question(transcript, question):
    return qa_pipeline(
        question=question,
        context=transcript[:1024]  # Model's max context
    )['answer']


# agent.py - Add translation functionality
from transformers import MBart50TokenizerFast, MBartForConditionalGeneration

translation_model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
translation_tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

LANGUAGE_MAPPING = {
    "French": "fr_XX",
    "Spanish": "es_XX",
    "German": "de_DE",
    "Chinese": "zh_CN",
    "Hindi": "hi_IN",
    "Bengali": "bn_IN"
}

def translate_summary(summary, target_lang):
    translation_tokenizer.src_lang = "en_XX"
    inputs = translation_tokenizer(
        summary, 
        return_tensors="pt", 
        truncation=True, 
        max_length=1024
    )
    
    generated_tokens = translation_model.generate(
        **inputs,
        forced_bos_token_id=translation_tokenizer.lang_code_to_id[target_lang]
    )
    
    return translation_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]

from app.backend.visualizer import VisualSummarizer

visualizer = VisualSummarizer()

def generate_mindmap(transcript):
    try:
        return visualizer.create_mindmap(transcript)
    except Exception as e:
        return f"Mindmap generation failed: {str(e)}"


File src > app > backend > utils.py:
# backend/utils.py (updated)
from youtube_transcript_api import YouTubeTranscriptApi
from langdetect import detect, DetectorFactory
import re
from pytube import YouTube
import whisper
import tempfile
import os

# For consistent results
DetectorFactory.seed = 0

def extract_video_id(url):
    regex = r"(?:v=|\/)([0-9A-Za-z_-]{11}).*"
    match = re.search(regex, url)
    return match.group(1) if match else None

def get_transcript_with_lang(url):
    video_id = extract_video_id(url)
    try:
        # Get available transcripts
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        
        # Try to find manual transcript first
        try:
            transcript = transcript_list.find_manually_created_transcript()
        except:
            # Fallback to auto-generated transcript
            transcript = transcript_list.find_generated_transcript([transcript.language_code for transcript in transcript_list])
        
        # Get text and language
        transcript_text = " ".join([entry['text'] for entry in transcript.fetch()])
        return transcript_text, transcript.language_code
    
    except Exception as e:
        # Fallback to language detection from text
        transcript = YouTubeTranscriptApi.get_transcript(video_id)
        transcript_text = " ".join([entry['text'] for entry in transcript])
        return transcript_text, detect(transcript_text)

def get_transcript(url):
    try:
        # First try YouTube captions
        video_id = extract_video_id(url)
        transcript = YouTubeTranscriptApi.get_transcript(video_id)
        return " ".join([entry['text'] for entry in transcript])
    except:
        # Fallback to audio transcription
        try:
            audio_path = download_audio(url)
            return transcribe_audio(audio_path)
        except Exception as e:
            raise ValueError(f"Both methods failed: {str(e)}")

def download_audio(url):
    try:
        yt = YouTube(url)
        audio_stream = yt.streams.filter(only_audio=True).first()
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
        audio_stream.download(filename=temp_file.name)
        return temp_file.name
    except Exception as e:
        raise ValueError(f"Audio download failed: {str(e)}")

def transcribe_audio(audio_path):
    try:
        model = whisper.load_model("base")  # Use 'small' or 'medium' for better accuracy
        result = model.transcribe(audio_path)
        return result["text"]
    except Exception as e:
        raise ValueError(f"Transcription failed: {str(e)}")
    finally:
        if os.path.exists(audio_path):
            os.remove(audio_path)


File src > app > backend > visualizer.py:
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
import networkx as nx
import matplotlib.pyplot as plt
import community.community_louvain as cl
from transformers import pipeline
import tempfile
import os

class VisualSummarizer:
    def __init__(self):
        self.kw_model = KeyBERT(model=SentenceTransformer("all-MiniLM-L6-v2"))
        self.ner_pipeline = pipeline(
            "ner",
            model="dslim/bert-base-NER",
            aggregation_strategy="simple"
        )

    def create_mindmap(self, text, max_nodes=15):
        keywords = self._extract_keywords(text)
        filtered_keys = self._filter_keywords(keywords, max_nodes)
        graph = self._build_graph(filtered_keys, text)
        return self._visualize_graph(graph)

    def _extract_keywords(self, text):
        kw_results = self.kw_model.extract_keywords(
            text, 
            keyphrase_ngram_range=(1, 2),
            stop_words='english',
            top_n=20
        )
        ner_results = self.ner_pipeline(text)
        combined = {kw[0]: kw[1] for kw in kw_results}
        for entity in ner_results:
            combined[entity['word']] = entity['score']
        return combined

    def _filter_keywords(self, keywords, max_nodes):
        sorted_keys = sorted(keywords.items(), key=lambda x: x[1], reverse=True)
        return [k[0] for k in sorted_keys[:max_nodes]]

    def _build_graph(self, keywords, text):
        G = nx.Graph()
        text_lower = text.lower()
        for kw in keywords:
            G.add_node(kw)
        for i, kw1 in enumerate(keywords):
            for kw2 in keywords[i+1:]:
                if f' {kw1.lower()} ' in text_lower and f' {kw2.lower()} ' in text_lower:
                    G.add_edge(kw1, kw2)
        return G

    def _visualize_graph(self, graph):
        plt.figure(figsize=(12, 8))
        if len(graph.nodes) > 0:
            partition = cl.best_partition(graph)
            pos = nx.spring_layout(graph, k=0.5)
            nx.draw_networkx_nodes(graph, pos, node_size=2500, cmap=plt.cm.RdYlBu, 
                                node_color=list(partition.values()))
            nx.draw_networkx_edges(graph, pos, alpha=0.5)
            nx.draw_networkx_labels(graph, pos, font_size=9, font_family='sans-serif')
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
        plt.savefig(temp_file.name, bbox_inches='tight')
        plt.close()
        return temp_file.name


File src > app > frontend > __init__.py:


File src > app > frontend > gradio_app.py:
import gradio as gr
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from app.backend.agent import generate_summary, answer_question, translate_summary, generate_mindmap
from app.backend.utils import *

custom_theme = gr.themes.Default(
    primary_hue="blue",
    secondary_hue="slate",
    neutral_hue="gray"
).set(
    body_background_fill="white",
    block_background_fill="white",
    button_primary_background_fill="#000000",
    button_primary_text_color="white",
    button_primary_background_fill_hover="#1a1a1a",
    block_label_background_fill="#000000",
    block_label_text_color="white",
    body_text_color_subdued="black"
)

css = """
.gradio-container {
    max-width: 100vw !important;
    width: 100vw !important;
    min-width: 0 !important;
    margin: 0 !important;
    padding: 0 !important;
}
footer {visibility: hidden}
@import url('https://fonts.boomla.com/bangla.css');
body {font-family: 'SolaimanLipi', 'Segoe UI', Arial, sans-serif!important;}
#main-card {
    background: #fff;
    border-radius: 18px;
    box-shadow: 0 4px 28px #0001;
    padding: 30px 32px 32px 32px;
    margin-top: 40px;
    max-width: 900px;
    margin-left: auto;
    margin-right: auto;
}
.section-title {font-size: 1.2em; font-weight: 600; margin-bottom: 10px; color: #2c3e50;}
#summary_output, #translated_output {
    background: #111;
    color: #fff;
    border-radius: 10px;
    padding: 18px;
    min-height: 120px;
    font-size: 1.08em;
    border: 1.5px solid #1976d2;
    box-shadow: 0 2px 8px #0001;
    word-break: break-word;
}
#mindmap_card {background: #f3f7fb; border-radius: 10px; padding: 18px;}
#detected_lang {font-size: 0.95em; color: #888; margin-bottom: 10px;}
.copy-btn {
    background: #1976d2!important;
    color: #fff!important;
    border: none!important;
    border-radius: 8px!important;
    padding: 7px 18px!important;
    margin-left: 8px!important;
    font-weight: 600!important;
    font-size: 1em!important;
    box-shadow: 0 2px 8px #0002;
    transition: background 0.2s;
}
.copy-btn:hover {
    background: #1565c0!important;
}
#footer {text-align: center; color: #aaa; margin-top: 38px; font-size: 0.93em;}
@media (max-width: 600px) {
  #main-card {padding: 10px;}
}
"""

LANGUAGE_MAPPING = {
    "English": "en_XX",
    "French": "fr_XX",
    "Spanish": "es_XX", 
    "German": "de_DE",
    "Chinese": "zh_CN",
    "Hindi": "hi_IN",
    "Arabic": "ar_AR",
    "Russian": "ru_RU",
    "Bengali": "bn_IN"
}

def analyze_video_handler(url, mode):
    if not (url.startswith("https://www.youtube.com/") or url.startswith("https://youtu.be/")):
        raise gr.Error("Invalid YouTube URL format")
    try:
        transcript = get_transcript(url)
        summary = generate_summary(transcript, mode.lower())
        mindmap_path = generate_mindmap(summary)
        summary_md = f"### {mode} Summary\n\n" + summary.replace('\n', '\n\n')
        return summary_md, mindmap_path, ""
    except Exception as e:
        return f"**Analysis failed:** {str(e)}", None, ""

def translate_handler(summary_md, target_lang):
    import re
    summary = re.sub(r"^#+.*\n", "", summary_md)
    if not summary or summary.lower().startswith("**analysis failed"):
        raise gr.Error("Generate a valid summary first")
    lang_code = LANGUAGE_MAPPING.get(target_lang, "en_XX")
    try:
        translated = translate_summary(summary, lang_code)
        return f"### {target_lang} Translation\n\n{translated.replace(chr(10), chr(10)+chr(10))}"
    except Exception as e:
        return f"**Translation failed:** {str(e)}"

copy_js = """
async (text) => {
    try {
        await navigator.clipboard.writeText(text);
        return "Copied!";
    } catch (e) {
        return "Copy failed";
    }
}
"""

with gr.Blocks(theme=custom_theme, css=css) as app:
    gr.Markdown("# YouTube AI Analyzer 3.0 🚀")
    with gr.Column(elem_id="main-card"):
        with gr.Tab("Video Analysis"):
            gr.Markdown('<div class="section-title">Step 1: Paste YouTube Link</div>')
            url_input = gr.Textbox(
                label="YouTube URL",
                placeholder="Paste video URL here...",
                info="Supports both https://www.youtube.com/ and https://youtu.be/ links."
            )
            with gr.Row():
                mode_dropdown = gr.Radio(
                    choices=["Short", "Medium", "Detailed"],
                    value="Medium",
                    label="Summary Length",
                    interactive=True
                )
                analyze_btn = gr.Button("🔍 Analyze Video", variant="primary")
            with gr.Row():
                with gr.Column(scale=2):
                    gr.Markdown('<div class="section-title">Summary</div>')
                    detected_lang = gr.Markdown("", elem_id="detected_lang")
                    summary_output = gr.Markdown(elem_id="summary_output")
                    with gr.Row():
                        copy_summary_btn = gr.Button("Copy", elem_id="copy_summary", elem_classes="copy-btn")
                with gr.Column(scale=1):
                    gr.Markdown('<div class="section-title">Mind Map</div>')
                    with gr.Column(elem_id="mindmap_card"):
                        vis_output = gr.Image(
                            show_label=False,
                            height=220,
                            width=320,
                            show_download_button=True
                        )
            with gr.Accordion("🌐 Translate Summary", open=False):
                with gr.Row():
                    lang_dropdown = gr.Dropdown(
                        choices=list(LANGUAGE_MAPPING.keys()),
                        value="English",
                        label="Target Language",
                        interactive=True
                    )
                    translate_btn = gr.Button("🌐 Translate")
                translated_output = gr.Markdown(elem_id="translated_output")
                with gr.Row():
                    copy_translated_btn = gr.Button("Copy", elem_id="copy_translated", elem_classes="copy-btn")
        gr.Markdown(
            """
            <div id="footer">
            <b>Tips:</b> Click "Copy" to copy summaries. Download the mind map for your notes.<br>
            <i>Powered by Gradio, Transformers, and mBART.</i>
            </div>
            """
        )

    analyze_btn.click(
        fn=analyze_video_handler,
        inputs=[url_input, mode_dropdown],
        outputs=[summary_output, vis_output, detected_lang],
        show_progress="full"
    )
    translate_btn.click(
        fn=translate_handler,
        inputs=[summary_output, lang_dropdown],
        outputs=translated_output
    )
    copy_summary_btn.click(None, inputs=summary_output, outputs=None, js=copy_js)
    copy_translated_btn.click(None, inputs=translated_output, outputs=None, js=copy_js)

if __name__ == "__main__":
    app.launch()


File src > app > __init__.py:

File __init__.py:

File .env:
OPENAI_API_KEY=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
#MODEL_NAME = "sshleifer/distilbart-cnn-12-6"
MODEL_NAME = "facebook/bart-large-cnn"
ALLOWED_DOMAINS=www.youtube.com,youtu.be

File requirements.txt:
# Keep only these core dependencies
youtube-transcript-api
setuptools>=68.0.0
gradio>=4.25.0
transformers>=4.37.0
torch>=2.2.2
accelerate>=0.31.0
sentencepiece
protobuf
langdetect==1.0.9
pytube==15.0.0
openai-whisper==20231117
ffmpeg-python==0.2.0
keybert==0.7.0
sentence-transformers==2.7.0
networkx==3.2.1
matplotlib==3.8.2
python-louvain==0.16
python-dotenv


