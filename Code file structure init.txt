Code structure:

.
‚îú‚îÄ‚îÄ assests
‚îÇ   ‚îî‚îÄ‚îÄ hermes.png
‚îú‚îÄ‚îÄ Code file structure init.txt
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ mindmap.png
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ run.sh
‚îú‚îÄ‚îÄ run_test.sh
‚îî‚îÄ‚îÄ src
    ‚îî‚îÄ‚îÄ app
        ‚îú‚îÄ‚îÄ backend
        ‚îÇ   ‚îú‚îÄ‚îÄ agent.py
        ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
        ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.cpython-312.pyc
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.cpython-312.pyc
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils.cpython-312.pyc
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualizer.cpython-312.pyc
        ‚îÇ   ‚îú‚îÄ‚îÄ utils.py
        ‚îÇ   ‚îî‚îÄ‚îÄ visualizer.py
        ‚îú‚îÄ‚îÄ frontend
        ‚îÇ   ‚îú‚îÄ‚îÄ gradio_app.py
        ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
        ‚îÇ   ‚îî‚îÄ‚îÄ __pycache__
        ‚îÇ       ‚îú‚îÄ‚îÄ gradio_app.cpython-312.pyc
        ‚îÇ       ‚îî‚îÄ‚îÄ __init__.cpython-312.pyc
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ __pycache__
            ‚îî‚îÄ‚îÄ __init__.cpython-312.pyc


File src > app > backend > __init__.py:


File src > app > backend > agent.py:
from transformers import (
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    MBart50TokenizerFast,
    MBartForConditionalGeneration,
    pipeline,
    AutoModelForSeq2SeqLM,
)
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import torch


class QAAgent:
    def __init__(self):
        # QA System
        self.embedder = SentenceTransformer("all-MiniLM-L6-v2")
        qa_model = AutoModelForQuestionAnswering.from_pretrained(
            "deepset/roberta-large-squad2"
        )
        qa_tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-large-squad2")
        self.qa_model = pipeline(
            "question-answering",
            model=qa_model,
            tokenizer=qa_tokenizer,
            max_seq_len=512,
            device=0 if torch.cuda.is_available() else -1,
        )
        self.cache = {}

        # Translation System
        self.translator = MBartForConditionalGeneration.from_pretrained(
            "facebook/mbart-large-50-many-to-many-mmt"
        )
        self.trans_tokenizer = MBart50TokenizerFast.from_pretrained(
            "facebook/mbart-large-50-many-to-many-mmt"
        )
        self.lang_map = {
            "English": "en_XX",
            "Bengali": "bn_IN",
            "Hindi": "hi_IN",
            "Chinese": "zh_CN",
            "Spanish": "es_XX",
            "French": "fr_XX",
            "Arabic": "ar_AR",
            "Russian": "ru_RU",
        }
        self.question_generator = pipeline(
            "text2text-generation",
            model="mrm8488/t5-base-finetuned-question-generation-ap",
            device=0 if torch.cuda.is_available() else -1,
            torch_dtype=torch.float32
        )

    # app/backend/agent.py
    def generate_questions(self, text, num_questions=3):
        prompt = f"generate questions: {text[:3000]}"
        results = self.question_generator(
            prompt,
            max_length=50,
            num_beams=5,  # Enable beam search
            num_return_sequences=num_questions,
            early_stopping=True
        )
        return [q['generated_text'].strip() for q in results if '?' in q['generated_text']]


    def chunk_text(self, text, chunk_size=300):
        """Enhanced chunking with overlap"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0

        for i in range(len(words)):
            current_chunk.append(words[i])
            current_length += 1

            if current_length >= chunk_size:
                chunks.append(" ".join(current_chunk))
                # Keep 20% overlap
                current_chunk = current_chunk[-int(chunk_size * 0.2) :]
                current_length = len(current_chunk)

        if current_chunk:
            chunks.append(" ".join(current_chunk))
        return chunks

    def process_transcript(self, url, transcript):
        video_id = url.split("v=")[-1]
        if video_id not in self.cache:
            chunks = self.chunk_text(transcript)
            self.cache[video_id] = {
                "chunks": chunks,
                "embeddings": self.embedder.encode(chunks),
            }

    def answer_question(self, url, question):
        video_id = url.split("v=")[-1]
        if video_id not in self.cache:
            raise ValueError("Process transcript first")

        data = self.cache[video_id]
        question_embed = self.embedder.encode([question])

        similarities = cosine_similarity(question_embed, data["embeddings"])[0]

        # Handle case with few chunks
        valid_k = min(5, len(similarities))
        if valid_k == 0:
            return "‚ùå Not enough context to answer this question"

        top_idxs = np.argsort(similarities)[
            -valid_k:
        ]  # Use argsort instead of argpartition
        context = " ".join([data["chunks"][i] for i in top_idxs][-3:])

        if len(context.split()) > 500:
            context = " ".join(context.split()[:500])

        result = self.qa_model(
            question=question,
            context=context,
            max_answer_len=150,
            handle_impossible_answer=True,
        )

        if (
            result["answer"].strip() == "" or result["score"] < 0.12
        ):  # Slightly lower threshold
            return f"I'm unsure, but here's a relevant excerpt: {context[:200]}..."
        return f"{result['answer']} (Confidence: {result['score']:.0%})"

    def translate_text(self, text, target_lang):
        if target_lang not in self.lang_map:
            raise ValueError(f"Unsupported language: {target_lang}")

        self.trans_tokenizer.src_lang = "en_XX"
        encoded = self.trans_tokenizer(text, return_tensors="pt")
        generated_tokens = self.translator.generate(
            **encoded,
            forced_bos_token_id=self.trans_tokenizer.lang_code_to_id[
                self.lang_map[target_lang]
            ],
        )
        return self.trans_tokenizer.batch_decode(
            generated_tokens, skip_special_tokens=True
        )[0]


# Explicitly load tokenizer with model_max_length
summarizer_tokenizer = AutoTokenizer.from_pretrained(
    "facebook/bart-large-cnn"
    # model_max_length=1024  # BART's actual max input length
)
summarizer_model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn")

summarizer = pipeline(
    "summarization",
    model=summarizer_model,
    tokenizer=summarizer_tokenizer,
    device=0 if torch.cuda.is_available() else -1,
)


def chunk_text(text, max_tokens=900):
    """Split text by tokens instead of words"""
    tokenizer = summarizer_tokenizer  # Reuse the initialized tokenizer
    tokens = tokenizer.encode(text, truncation=False, add_special_tokens=False)

    for i in range(0, len(tokens), max_tokens):
        chunk = tokens[i : i + max_tokens]
        yield tokenizer.decode(chunk, skip_special_tokens=True)


def generate_summary(transcript, mode):
    # Set summary lengths based on mode
    if mode.lower() == "short":
        max_length, min_length = 40, 20
    elif mode.lower() == "medium":
        max_length, min_length = 100, 40
    else:  # detailed
        max_length, min_length = 150, 60

    # Always chunk transcript if too long
    summaries = []
    for chunk in chunk_text(transcript, max_tokens=900):
        # Explicitly truncate input to 1024 tokens
        inputs = summarizer_tokenizer(
            chunk, max_length=1024, truncation=True, return_tensors="pt"
        )
        truncated_chunk = summarizer_tokenizer.decode(inputs["input_ids"][0])

        summary = summarizer(
            truncated_chunk,  # Use truncated input
            max_length=max_length,
            min_length=min_length,
            do_sample=False,
        )[0]["summary_text"]
        summaries.append(summary)

    # If multiple chunks, summarize the summaries
    if len(summaries) > 1:
        combined = " ".join(summaries)
        final_summary = summarizer(
            combined,
            max_length=max_length,
            min_length=min_length,
            do_sample=False,
            truncation=True,
        )[0]["summary_text"]
        return final_summary
    else:
        return summaries[0]


def generate_mindmap(summary):
    """Generate mindmap visualization"""
    words = [word for word in summary.split() if len(word) > 3][:15]
    G = nx.Graph()
    for i, word in enumerate(words):
        G.add_node(word)
        if i > 0:
            G.add_edge(words[i - 1], word)

    plt.figure(figsize=(16, 10))
    pos = nx.spring_layout(G, k=0.5)
    nx.draw(
        G,
        pos,
        with_labels=True,
        node_size=2500,
        node_color="skyblue",
        font_size=10,
        font_weight="bold",
        edge_color="gray",
    )
    plt.savefig("mindmap.png", bbox_inches="tight")
    plt.close()
    return "mindmap.png"


qa_agent = QAAgent()



File src > app > backend > utils.py:
import requests
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound

def get_video_id(url):
    # Handles both youtu.be and youtube.com URLs
    if "youtu.be/" in url:
        return url.split("youtu.be/")[-1].split("?")[0]
    elif "youtube.com/watch?v=" in url:
        return url.split("v=")[-1].split("&")[0]
    return None

def get_transcript(url):
    try:
        video_id = get_video_id(url)
        if not video_id:
            raise ValueError("Invalid YouTube URL")
        
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        transcript = transcript_list.find_generated_transcript(['en'])
        
        # Access text via object attribute instead of dictionary key
        return " ".join([snippet.text for snippet in transcript.fetch()])
        
    except (TranscriptsDisabled, NoTranscriptFound) as e:
        raise RuntimeError(f"Transcript unavailable: {str(e)}")
    except Exception as e:
        raise RuntimeError(f"Transcript error: {str(e)}")

def get_video_metadata(url):
    try:
        response = requests.get(
            f"https://www.youtube.com/oembed?url={url}&format=json"
        )
        if response.status_code == 200:
            data = response.json()
            return {
                "title": data.get("title", "No title"),
                "channel": data.get("author_name", "Unknown channel"),
                "thumbnail_url": data.get("thumbnail_url", "")
            }
        return None
    except Exception as e:
        print(f"Metadata error: {e}")
        return None

File src > app > backend > visualizer.py:
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
import networkx as nx
import matplotlib.pyplot as plt
import community.community_louvain as cl
from transformers import pipeline
import tempfile


class VisualSummarizer:
    def __init__(self):
        self.kw_model = KeyBERT(model=SentenceTransformer("all-MiniLM-L6-v2"))
        self.ner_pipeline = pipeline(
            "ner", model="dslim/bert-base-NER", aggregation_strategy="simple"
        )

    def create_mindmap(self, text, max_nodes=15):
        keywords = self._extract_keywords(text)
        filtered_keys = self._filter_keywords(keywords, max_nodes)
        graph = self._build_graph(filtered_keys, text)
        return self._visualize_graph(graph)

    def _extract_keywords(self, text):
        kw_results = self.kw_model.extract_keywords(
            text, keyphrase_ngram_range=(1, 2), stop_words="english", top_n=20
        )
        ner_results = self.ner_pipeline(text)
        combined = {kw[0]: kw[1] for kw in kw_results}
        for entity in ner_results:
            combined[entity["word"]] = entity["score"]
        return combined

    def _filter_keywords(self, keywords, max_nodes):
        sorted_keys = sorted(keywords.items(), key=lambda x: x[1], reverse=True)
        return [k[0] for k in sorted_keys[:max_nodes]]

    def _build_graph(self, keywords, text):
        G = nx.Graph()
        text_lower = text.lower()
        for kw in keywords:
            G.add_node(kw)
        for i, kw1 in enumerate(keywords):
            for kw2 in keywords[i + 1 :]:
                if (
                    f" {kw1.lower()} " in text_lower
                    and f" {kw2.lower()} " in text_lower
                ):
                    G.add_edge(kw1, kw2)
        return G

    def _visualize_graph(self, graph):
        plt.figure(figsize=(12, 8))
        if len(graph.nodes) > 0:
            partition = cl.best_partition(graph)
            pos = nx.spring_layout(graph, k=0.5)
            nx.draw_networkx_nodes(
                graph,
                pos,
                node_size=2500,
                cmap=plt.cm.RdYlBu,
                node_color=list(partition.values()),
            )
            nx.draw_networkx_edges(graph, pos, alpha=0.5)
            nx.draw_networkx_labels(graph, pos, font_size=9, font_family="sans-serif")
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
        plt.savefig(temp_file.name, bbox_inches="tight")
        plt.close()
        return temp_file.name


File src > app > frontend > __init__.py:

File src > app > frontend > gradio_app.py:
import gradio as gr
import sys
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from app.backend.agent import generate_summary, generate_mindmap, qa_agent
from app.backend.utils import get_transcript, get_video_metadata

# Add at the top of your gradio_app.py
import warnings
from transformers import logging
logging.set_verbosity_error()  # Suppress all transformers warnings
warnings.filterwarnings("ignore", category=UserWarning)  # General warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

custom_theme = gr.themes.Default(primary_hue="blue", secondary_hue="slate").set(
    button_primary_background_fill="#1976d2",
    button_primary_text_color="white",
    button_primary_background_fill_hover="#1565c0",
)

css = """
.gradio-container {max-width: 100vw!important; margin: 0!important}
footer {visibility: hidden}
#main-card {
    background: white; 
    border-radius: 16px;
    box-shadow: 0 4px 24px #0001;
    padding: 2.4rem;
    margin: 2rem auto;
    max-width: 900px;
}
#summary_output, #translated_output {
    background: #000 !important;
    color: #fff !important;
    padding: 20px;
    border-radius: 12px;
    margin: 15px 0;
}
#mindmap-output img {
    max-height: 70vh !important;
    width: 100% !important;
    object-fit: contain;
}
.copy-btn {
    background: #1976d2!important;
    color: white!important;
    margin: 10px 0;
}
.dark .copy-btn {
    background: #1565c0!important;
}
button.suggested-question {
    max-width: 300px;
    white-space: normal;
    line-height: 1.2;
    padding: 8px 12px;
    margin: 4px;
    flex-grow: 1;
    text-align: left;
    height: auto;
}
"""

def analyze_video(url, mode):
    try:
        transcript = get_transcript(url)
        qa_agent.process_transcript(url, transcript)
        if not transcript:
            raise ValueError("Transcript is empty")
        summary = generate_summary(transcript, mode)
        mindmap = generate_mindmap(summary)
        return (f"### {mode.capitalize()} Summary\n\n{summary}", mindmap, "")
    except Exception as e:
        return f"**Error:** {str(e)}", None, ""

def handle_translation(summary, lang):
    try:
        translated = qa_agent.translate_text(summary, lang)
        return f"### {lang} Translation\n\n{translated}"
    except Exception as e:
        return f"**Translation Error:** {str(e)}"

def handle_qa(history, url, question):
    try:
        answer = qa_agent.answer_question(url, question)
        return history + [
            {"role": "user", "content": question},
            {"role": "assistant", "content": answer},
        ]
    except Exception as e:
        return history + [
            {"role": "user", "content": question},
            {"role": "assistant", "content": f"‚ö†Ô∏è Error: {str(e)}"},
        ]

with gr.Blocks(theme=custom_theme, css=css) as app:
    # ========== Header with Logo ==========
    gr.Image('assests/hermes.png', show_label=False, width=60, height=100, elem_id="logo")
    gr.Markdown('<h1 style="text-align:center;">Hermes AI</h1>')

    # ========== Metadata Preview ==========
    with gr.Row(variant="panel", visible=False) as preview_row:
        thumbnail = gr.Image(label="Video Thumbnail", width=200)
        with gr.Column():
            video_title = gr.Markdown()
            video_channel = gr.Markdown()

    # Hidden component to control visibility
    visibility_tracker = gr.Textbox(visible=False)
    visibility_state = gr.State(False)

    # ========== Main Card ==========
    with gr.Column(elem_id="main-card"):
        # Input Section
        with gr.Row():
            url_input = gr.Textbox(
                label="YouTube URL",
                placeholder="Paste video link here...",
                max_lines=1,
                scale=4,
            )
            mode_selector = gr.Radio(
                choices=["Short", "Medium", "Detailed"],
                value="Medium",
                label="Summary Mode",
                scale=2,
            )
            analyze_btn = gr.Button("Analyze Video", variant="primary", scale=1)

        # Summary Section
        summary_output = gr.Markdown(elem_id="summary_output")
        copy_summary_btn = gr.Button("üìã Copy Summary", elem_classes="copy-btn")

        # Translation Section
        with gr.Accordion("üåç Translation (Supports 8 Languages)", open=False):
            with gr.Row():
                lang_selector = gr.Dropdown(
                    choices=[
                        "English",
                        "Bengali",
                        "Hindi",
                        "Chinese",
                        "Spanish",
                        "French",
                        "Arabic",
                        "Russian",
                    ],
                    value="English",
                    label="Target Language",
                    scale=3,
                )
                translate_btn = gr.Button("Translate", scale=1)
            translated_output = gr.Markdown(elem_id="translated_output")
            copy_translate_btn = gr.Button(
                "üìã Copy Translation", elem_classes="copy-btn"
            )

        # Mind Map
        with gr.Accordion("üß† Interactive Mind Map", open=True):
            mindmap_output = gr.Image(
                label="Concept Visualization",
                elem_id="mindmap-output",
                show_download_button=True,
            )

        # Q&A Section
        with gr.Accordion("üí¨ AI-Powered Q&A", open=False):
            qa_chat = gr.Chatbot(
                height=400,
                avatar_images=(
                    "https://i.imgur.com/7kQEsHU.png",
                    "https://i.imgur.com/8EeSUQ3.png",
                ),
                show_label=False,
                type="messages",
            )
            
            # Suggested Questions
            suggested_questions_visible = gr.State(False)
            with gr.Row(visible=False) as suggested_questions_row:
                gr.Markdown("**Suggested Questions:**")
                suggested_btns = [
                    gr.Button(visible=False, elem_classes="suggested-question") 
                    for _ in range(3)
                ]
            
            # Input Section
            with gr.Row():
                qa_input = gr.Textbox(
                    placeholder="Ask anything about the video...",
                    show_label=False,
                    scale=4,
                )
                qa_btn = gr.Button("Ask AI", variant="secondary", scale=1)

    # ========== Metadata Preview Logic ==========
    def update_preview(url):
        if not url or not url.startswith(("https://www.youtube.com", "https://youtu.be")):
            return [None, "", "", False]
        
        metadata = get_video_metadata(url)
        if not metadata:
            return [
                None,
                "### Video Not Found",
                "The video may be private or unavailable",
                True
            ]
        
        return [
            metadata["thumbnail_url"],
            f"### {metadata['title']}",
            f"**Channel**: {metadata['channel']}",
            True
        ]

    url_input.input(
        fn=update_preview,
        inputs=url_input,
        outputs=[thumbnail, video_title, video_channel, visibility_tracker]
    )

    # Replace visibility_tracker logic with:
    def update_row_visibility(should_show):
        return gr.Row.update(visible=should_show)

    visibility_tracker.change(
        fn=update_row_visibility,
        inputs=visibility_tracker,
        outputs=preview_row
    )


    # ========== Suggested Questions Logic ==========
    def update_suggested_questions(summary):
        """Generate questions and return visibility state"""
        if not summary or "Error:" in summary:
            return [{"visible": False}]*3 + [False]
        
        try:
            # Your question generation logic here
            questions = ["Q1", "Q2", "Q3"]  # Replace with actual generated questions
            updates = []
            for i in range(3):
                updates.append({
                    "visible": i < len(questions),
                    "value": questions[i] if i < len(questions) else "",
                    "__type__": "update"
                })
            return updates + [len(questions) > 0]
        except Exception as e:
            return [{"visible": False}]*3 + [False]

    def update_row_visibility(visible):
        """Convert boolean to proper row update syntax"""
        return {"visible": visible, "__type__": "update"}

    # In your event handling:
    analyze_btn.click(
        analyze_video,
        inputs=[url_input, mode_selector],
        outputs=[summary_output, mindmap_output, translated_output]
    ).then(
        update_suggested_questions,
        inputs=summary_output,
        outputs=[*suggested_btns, visibility_state]
    ).then(
        update_row_visibility,
        inputs=visibility_state,
        outputs=suggested_questions_row
    )


    def use_suggested_question(question):
        """Insert question into input and clear suggestions"""
        return (
            question,  # Update qa_input
            {"visible": False, "__type__": "update"},  # Button 1
            {"visible": False, "__type__": "update"},  # Button 2
            {"visible": False, "__type__": "update"},  # Button 3
            {"visible": False, "__type__": "update"}   # Row visibility
        )

# Update button click handlers
    for btn in suggested_btns:
        btn.click(
            fn=use_suggested_question,
            inputs=btn,
            outputs=[qa_input, *suggested_btns, suggested_questions_row]
        ).then(
            fn=handle_qa,
            inputs=[qa_chat, url_input, qa_input],
            outputs=qa_chat
        )

    # ========== Event Handling ==========
    # Replace current analyze_btn.click() calls with:
    def clear_suggestions():
        return [{"visible": False}]*3 + [False]
    analyze_btn.click(
    fn=clear_suggestions,
    outputs=[*suggested_btns, suggested_questions_row]
).then(
    analyze_video,
    inputs=[url_input, mode_selector],
    outputs=[summary_output, mindmap_output, translated_output]
).then(
    fn=update_suggested_questions,
    inputs=summary_output,
    outputs=[*suggested_btns, suggested_questions_row]
)



    translate_btn.click(
        handle_translation,
        inputs=[summary_output, lang_selector],
        outputs=translated_output,
    )

    qa_btn.click(handle_qa, inputs=[qa_chat, url_input, qa_input], outputs=qa_chat)
    qa_input.submit(handle_qa, inputs=[qa_chat, url_input, qa_input], outputs=qa_chat)

    # Copy functionality
    copy_summary_btn.click(
        None,
        inputs=summary_output,
        outputs=None,
        js="async (text) => { await navigator.clipboard.writeText(text) }",
    )
    copy_translate_btn.click(
        None,
        inputs=translated_output,
        outputs=None,
        js="async (text) => { await navigator.clipboard.writeText(text) }",
    )

if __name__ == "__main__":
    app.launch()


File src > app > __init__.py:

File __init__.py:

File .env:
OPENAI_API_KEY=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
#MODEL_NAME = "sshleifer/distilbart-cnn-12-6"
MODEL_NAME = "facebook/bart-large-cnn"
ALLOWED_DOMAINS=www.youtube.com,youtu.be

File requirements.txt:
gradio>=4.0.0
youtube-transcript-api>=0.6.1
transformers>=4.30.0
sentence-transformers>=2.7.0
scikit-learn>=1.3.2
torch>=2.0.0
numpy>=1.24.3
networkx>=3.1
matplotlib>=3.7.0
keybert>=0.7.0
python-louvain>=0.16
requests>=2.26.0
sentencepiece