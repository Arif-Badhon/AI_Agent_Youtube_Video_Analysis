Code structure:

.
â”œâ”€â”€ Code file structure init.txt
â”œâ”€â”€ __init__.py
â”œâ”€â”€ mindmap.png
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ src
    â””â”€â”€ app
        â”œâ”€â”€ backend
        â”‚   â”œâ”€â”€ agent.py
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ __pycache__
        â”‚   â”‚   â”œâ”€â”€ agent.cpython-312.pyc
        â”‚   â”‚   â”œâ”€â”€ __init__.cpython-312.pyc
        â”‚   â”‚   â”œâ”€â”€ utils.cpython-312.pyc
        â”‚   â”‚   â””â”€â”€ visualizer.cpython-312.pyc
        â”‚   â”œâ”€â”€ utils.py
        â”‚   â””â”€â”€ visualizer.py
        â”œâ”€â”€ frontend
        â”‚   â”œâ”€â”€ gradio_app.py
        â”‚   â””â”€â”€ __init__.py
        â”œâ”€â”€ __init__.py
        â””â”€â”€ __pycache__
            â””â”€â”€ __init__.cpython-312.pyc


File src > app > backend > __init__.py:


File src > app > backend > agent.py:
from transformers import (
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    MBart50TokenizerFast,
    MBartForConditionalGeneration,
    pipeline
)
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
import networkx as nx
import matplotlib.pyplot as plt

class QAAgent:
    def __init__(self):
        # QA System
        self.embedder = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
        qa_model = AutoModelForQuestionAnswering.from_pretrained("deepset/roberta-large-squad2")
        qa_tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-large-squad2")
        self.qa_model = pipeline(
            "question-answering",
            model=qa_model,
            tokenizer=qa_tokenizer
        )
        self.cache = {}

        # Translation System
        self.translator = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
        self.trans_tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
        self.lang_map = {
            "English": "en_XX",
            "Bengali": "bn_IN",
            "Hindi": "hi_IN",
            "Chinese": "zh_CN",
            "Spanish": "es_XX",
            "French": "fr_XX",
            "Arabic": "ar_AR",
            "Russian": "ru_RU"
        }

    def chunk_text(self, text, chunk_size=500):
        """Improved context-preserving chunking"""
        paragraphs = re.split(r'\n+', text)
        chunks = []
        current_chunk = []
        current_length = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            para_length = len(para.split())
            if current_length + para_length <= chunk_size:
                current_chunk.append(para)
                current_length += para_length
            else:
                chunks.append('\n'.join(current_chunk))
                current_chunk = [para]
                current_length = para_length
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        return chunks

    def process_transcript(self, url, transcript):
        video_id = url.split("v=")[-1]
        if video_id not in self.cache:
            chunks = self.chunk_text(transcript)
            self.cache[video_id] = {
                'chunks': chunks,
                'embeddings': self.embedder.encode(chunks)
            }

    def answer_question(self, url, question):
        video_id = url.split("v=")[-1]
        if video_id not in self.cache:
            raise ValueError("Process transcript first")
            
        data = self.cache[video_id]
        question_embed = self.embedder.encode([question])
        
        similarities = cosine_similarity(question_embed, data['embeddings'])[0]
        top_k = min(5, len(similarities))  # Dynamic chunk selection
        top_idxs = np.argpartition(similarities, -top_k)[-top_k:]
        context = " ".join([data['chunks'][i] for i in top_idxs])
        
        result = self.qa_model(
            question=question,
            context=context,
            max_answer_len=150,
            handle_impossible_answer=True
        )
        
        if result['answer'] == "" or result['score'] < 0.1:
            return "ðŸ” This information isn't clearly covered in the video."
        return f"{result['answer']} (Confidence: {result['score']:.0%})"

    def translate_text(self, text, target_lang):
        if target_lang not in self.lang_map:
            raise ValueError(f"Unsupported language: {target_lang}")
            
        self.trans_tokenizer.src_lang = "en_XX"
        encoded = self.trans_tokenizer(text, return_tensors="pt")
        generated_tokens = self.translator.generate(
            **encoded,
            forced_bos_token_id=self.trans_tokenizer.lang_code_to_id[self.lang_map[target_lang]]
        )
        return self.trans_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]

def generate_summary(transcript, mode):
    """Your summary logic (replace with actual implementation)"""
    return f"{mode.capitalize()} summary: {transcript[:500]}..."

def generate_mindmap(summary):
    """Generate mindmap visualization"""
    words = [word for word in summary.split() if len(word) > 3][:15]
    G = nx.Graph()
    for i, word in enumerate(words):
        G.add_node(word)
        if i > 0:
            G.add_edge(words[i-1], word)
    
    plt.figure(figsize=(16, 10))
    pos = nx.spring_layout(G, k=0.5)
    nx.draw(
        G, pos, with_labels=True, 
        node_size=2500, node_color="skyblue",
        font_size=10, font_weight="bold",
        edge_color="gray"
    )
    plt.savefig("mindmap.png", bbox_inches="tight")
    plt.close()
    return "mindmap.png"

qa_agent = QAAgent()


File src > app > backend > utils.py:
from youtube_transcript_api import YouTubeTranscriptApi
import re

def get_transcript(url):
    """Fetch and concatenate YouTube video transcript."""
    video_id = extract_video_id(url)
    try:
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
        return " ".join([t['text'] for t in transcript_list])
    except Exception as e:
        raise Exception(f"Transcript error: {str(e)}")

def extract_video_id(url):
    """Extract YouTube video ID from URL."""
    regex = r"(?:v=|\/)([0-9A-Za-z_-]{11}).*"
    matches = re.search(regex, url)
    if not matches:
        raise ValueError("Invalid YouTube URL")
    return matches.group(1)


File src > app > backend > visualizer.py:
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
import networkx as nx
import matplotlib.pyplot as plt
import community.community_louvain as cl
from transformers import pipeline
import tempfile
import os

class VisualSummarizer:
    def __init__(self):
        self.kw_model = KeyBERT(model=SentenceTransformer("all-MiniLM-L6-v2"))
        self.ner_pipeline = pipeline(
            "ner",
            model="dslim/bert-base-NER",
            aggregation_strategy="simple"
        )

    def create_mindmap(self, text, max_nodes=15):
        keywords = self._extract_keywords(text)
        filtered_keys = self._filter_keywords(keywords, max_nodes)
        graph = self._build_graph(filtered_keys, text)
        return self._visualize_graph(graph)

    def _extract_keywords(self, text):
        kw_results = self.kw_model.extract_keywords(
            text, 
            keyphrase_ngram_range=(1, 2),
            stop_words='english',
            top_n=20
        )
        ner_results = self.ner_pipeline(text)
        combined = {kw[0]: kw[1] for kw in kw_results}
        for entity in ner_results:
            combined[entity['word']] = entity['score']
        return combined

    def _filter_keywords(self, keywords, max_nodes):
        sorted_keys = sorted(keywords.items(), key=lambda x: x[1], reverse=True)
        return [k[0] for k in sorted_keys[:max_nodes]]

    def _build_graph(self, keywords, text):
        G = nx.Graph()
        text_lower = text.lower()
        for kw in keywords:
            G.add_node(kw)
        for i, kw1 in enumerate(keywords):
            for kw2 in keywords[i+1:]:
                if f' {kw1.lower()} ' in text_lower and f' {kw2.lower()} ' in text_lower:
                    G.add_edge(kw1, kw2)
        return G

    def _visualize_graph(self, graph):
        plt.figure(figsize=(12, 8))
        if len(graph.nodes) > 0:
            partition = cl.best_partition(graph)
            pos = nx.spring_layout(graph, k=0.5)
            nx.draw_networkx_nodes(graph, pos, node_size=2500, cmap=plt.cm.RdYlBu, 
                                node_color=list(partition.values()))
            nx.draw_networkx_edges(graph, pos, alpha=0.5)
            nx.draw_networkx_labels(graph, pos, font_size=9, font_family='sans-serif')
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
        plt.savefig(temp_file.name, bbox_inches='tight')
        plt.close()
        return temp_file.name


File src > app > frontend > __init__.py:

File src > app > frontend > gradio_app.py:
import gradio as gr
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from app.backend.agent import generate_summary, generate_mindmap, qa_agent
from app.backend.utils import get_transcript

# ========== Theme & CSS ==========
custom_theme = gr.themes.Default(
    primary_hue="blue",
    secondary_hue="slate"
).set(
    button_primary_background_fill="#1976d2",
    button_primary_text_color="white",
    button_primary_background_fill_hover="#1565c0",
)

css = """
.gradio-container {max-width: 100vw!important; margin: 0!important}
footer {visibility: hidden}
#main-card {
    background: white; 
    border-radius: 16px;
    box-shadow: 0 4px 24px #0001;
    padding: 2.4rem;
    margin: 2rem auto;
    max-width: 900px;
}
#summary_output, #translated_output {
    background: #000 !important;
    color: #fff !important;
    padding: 20px;
    border-radius: 12px;
    margin: 15px 0;
}
#mindmap-output img {
    max-height: 70vh !important;
    width: 100% !important;
    object-fit: contain;
}
.copy-btn {
    background: #1976d2!important;
    color: white!important;
    margin: 10px 0;
}
.dark .copy-btn {
    background: #1565c0!important;
}
"""

# ========== Handlers ==========
def analyze_video(url, mode):
    try:
        transcript = get_transcript(url)
        qa_agent.process_transcript(url, transcript)
        summary = generate_summary(transcript, mode)
        mindmap = generate_mindmap(summary)
        return (
            f"### {mode.capitalize()} Summary\n\n{summary}",
            mindmap,
            ""
        )
    except Exception as e:
        return f"**Error:** {str(e)}", None, ""

def handle_translation(summary, lang):
    try:
        translated = qa_agent.translate_text(summary, lang)
        return f"### {lang} Translation\n\n{translated}"
    except Exception as e:
        return f"**Translation Error:** {str(e)}"

def handle_qa(history, url, question):
    try:
        answer = qa_agent.answer_question(url, question)
        return history + [[question, answer]]
    except Exception as e:
        return history + [[question, f"âš ï¸ Error: {str(e)}"]]

# ========== Interface ==========
with gr.Blocks(theme=custom_theme, css=css) as app:
    gr.Markdown("# ðŸŽ¥ YouTube AI Analyzer 6.0")
    
    with gr.Column(elem_id="main-card"):
        # Input Section
        with gr.Row():
            url_input = gr.Textbox(
                label="YouTube URL",
                placeholder="Paste video link here...",
                max_lines=1,
                scale=4
            )
            mode_selector = gr.Radio(
                choices=["Short", "Medium", "Detailed"],
                value="Medium",
                label="Summary Mode",
                scale=2
            )
            analyze_btn = gr.Button("Analyze Video", variant="primary", scale=1)
        
        # Summary Section
        summary_output = gr.Markdown(elem_id="summary_output")
        copy_summary_btn = gr.Button("ðŸ“‹ Copy Summary", elem_classes="copy-btn")
        
        # Translation Section
        with gr.Accordion("ðŸŒ Translation (Supports 8 Languages)", open=False):
            with gr.Row():
                lang_selector = gr.Dropdown(
                    choices=["English", "Bengali", "Hindi", "Chinese",
                            "Spanish", "French", "Arabic", "Russian"],
                    value="English",
                    label="Target Language",
                    scale=3
                )
                translate_btn = gr.Button("Translate", scale=1)
            translated_output = gr.Markdown(elem_id="translated_output")
            copy_translate_btn = gr.Button("ðŸ“‹ Copy Translation", elem_classes="copy-btn")
        
        # Mind Map
        with gr.Accordion("ðŸ§  Interactive Mind Map", open=True):
            mindmap_output = gr.Image(
                label="Concept Visualization",
                elem_id="mindmap-output",
                show_download_button=True
            )
        
        # Q&A Section
        with gr.Accordion("ðŸ’¬ AI-Powered Q&A", open=False):
            qa_chat = gr.Chatbot(
                height=400,
                bubble_full_width=False,
                avatar_images=(
                    "https://i.imgur.com/7kQEsHU.png",  # User avatar
                    "https://i.imgur.com/8EeSUQ3.png"   # Bot avatar
                )
            )
            with gr.Row():
                qa_input = gr.Textbox(
                    placeholder="Ask anything about the video...",
                    show_label=False,
                    scale=4
                )
                qa_btn = gr.Button("Ask AI", variant="secondary", scale=1)

    # ========== Event Handling ==========
    analyze_btn.click(
        analyze_video,
        inputs=[url_input, mode_selector],
        outputs=[summary_output, mindmap_output, translated_output]
    )
    
    translate_btn.click(
        handle_translation,
        inputs=[summary_output, lang_selector],
        outputs=translated_output
    )
    
    qa_btn.click(
        handle_qa,
        inputs=[qa_chat, url_input, qa_input],
        outputs=qa_chat
    )
    qa_input.submit(
        handle_qa,
        inputs=[qa_chat, url_input, qa_input],
        outputs=qa_chat
    )
    
    # Copy functionality
    copy_summary_btn.click(
        None,
        inputs=summary_output,
        outputs=None,
        js="async (text) => { await navigator.clipboard.writeText(text) }"
    )
    copy_translate_btn.click(
        None,
        inputs=translated_output,
        outputs=None,
        js="async (text) => { await navigator.clipboard.writeText(text) }"
    )

if __name__ == "__main__":
    app.launch()


File src > app > __init__.py:

File __init__.py:

File .env:
OPENAI_API_KEY=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
#MODEL_NAME = "sshleifer/distilbart-cnn-12-6"
MODEL_NAME = "facebook/bart-large-cnn"
ALLOWED_DOMAINS=www.youtube.com,youtu.be

File requirements.txt:
gradio>=4.0.0
youtube-transcript-api>=0.6.1
transformers>=4.30.0
sentence-transformers>=2.7.0
scikit-learn>=1.3.2
torch>=2.0.0
numpy>=1.24.3
networkx>=3.1
matplotlib>=3.7.0
