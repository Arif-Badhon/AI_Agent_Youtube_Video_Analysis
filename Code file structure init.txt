Code file structure:
â”œâ”€â”€ __init__.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ src
    â””â”€â”€ app
        â”œâ”€â”€ backend
        â”‚Â Â  â”œâ”€â”€ agent.py
        â”‚Â Â  â”œâ”€â”€ __init__.py
        â”‚Â Â  â”œâ”€â”€ __pycache__
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ agent.cpython-312.pyc
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.cpython-312.pyc
        â”‚Â Â  â”‚Â Â  â””â”€â”€ utils.cpython-312.pyc
        â”‚Â Â  â””â”€â”€ utils.py
        â”œâ”€â”€ frontend
        â”‚Â Â  â”œâ”€â”€ gradio_app.py
        â”‚Â Â  â””â”€â”€ __init__.py
        â”œâ”€â”€ __init__.py
        â””â”€â”€ __pycache__
            â””â”€â”€ __init__.cpython-312.pyc

Here are the codes:
agent.py:
# backend/agent.py
#from openai import OpenAI
#from langchain.prompts import PromptTemplate
import os
from dotenv import load_dotenv
from transformers import pipeline
import torch
import whisper

# Load environment variables
load_dotenv()

#client = OpenAI(OPENAI_API_KEY=os.getenv("OPENAI_API_KEY"))

#client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

MODEL_NAME = os.getenv("MODEL_NAME")  # or your chosen model

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype="auto", device_map="auto")

summarizer = pipeline(
    "summarization",
    model=MODEL_NAME,
    tokenizer=MODEL_NAME,
    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available
    framework="pt"
)



summary_template = """Generate concise video summary with timestamps:
{transcript}
"""

qa_template = """Answer using video transcript:
Question: {question}
Transcript: {transcript}
"""
WHISPER_MODEL = "base"  # Change to "small", "medium", or "large" as needed

# Add this function
def get_whisper_model():
    return whisper.load_model(WHISPER_MODEL)


def generate_summary(transcript, mode="medium"):
    valid_modes = ["short", "medium", "detailed"]
    mode = mode.lower()
    if mode not in valid_modes:
        mode = "medium"
    
    mode_settings = {
        "short": {"max": 150, "min": 40, "beams": 4},
        "medium": {"max": 300, "min": 120, "beams": 5},
        "detailed": {"max": 600, "min": 200, "beams": 6}
    }
    
    config = mode_settings[mode]
    
    return summarizer(
        transcript,
        max_length=config["max"],
        min_length=config["min"],
        num_beams=config["beams"],
        no_repeat_ngram_size=3,
        do_sample=False,
        truncation=True
    )[0]['summary_text']




qa_pipeline = pipeline(
    "question-answering",
    model="deepset/roberta-base-squad2"
)

def answer_question(transcript, question):
    return qa_pipeline(
        question=question,
        context=transcript[:1024]  # Model's max context
    )['answer']


# agent.py - Add translation functionality
from transformers import MBart50TokenizerFast, MBartForConditionalGeneration

translation_model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
translation_tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

LANGUAGE_MAPPING = {
    "French": "fr_XX",
    "Spanish": "es_XX",
    "German": "de_DE",
    "Chinese": "zh_CN",
    "Hindi": "hi_IN",
    "Bengali": "bn_IN"
}

def translate_summary(summary, target_lang):
    translation_tokenizer.src_lang = "en_XX"
    inputs = translation_tokenizer(
        summary, 
        return_tensors="pt", 
        truncation=True, 
        max_length=1024
    )
    
    generated_tokens = translation_model.generate(
        **inputs,
        forced_bos_token_id=translation_tokenizer.lang_code_to_id[target_lang]
    )
    
    return translation_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]

from app.backend.visualizer import VisualSummarizer

visualizer = VisualSummarizer()

def generate_mindmap(transcript):
    return visualizer.create_mindmap(transcript)


utils.py:
# backend/utils.py (updated)
from youtube_transcript_api import YouTubeTranscriptApi
from langdetect import detect, DetectorFactory
import re
from pytube import YouTube
import whisper
import tempfile
import os

# For consistent results
DetectorFactory.seed = 0

def extract_video_id(url):
    regex = r"(?:v=|\/)([0-9A-Za-z_-]{11}).*"
    match = re.search(regex, url)
    return match.group(1) if match else None

def get_transcript_with_lang(url):
    video_id = extract_video_id(url)
    try:
        # Get available transcripts
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        
        # Try to find manual transcript first
        try:
            transcript = transcript_list.find_manually_created_transcript()
        except:
            # Fallback to auto-generated transcript
            transcript = transcript_list.find_generated_transcript([transcript.language_code for transcript in transcript_list])
        
        # Get text and language
        transcript_text = " ".join([entry['text'] for entry in transcript.fetch()])
        return transcript_text, transcript.language_code
    
    except Exception as e:
        # Fallback to language detection from text
        transcript = YouTubeTranscriptApi.get_transcript(video_id)
        transcript_text = " ".join([entry['text'] for entry in transcript])
        return transcript_text, detect(transcript_text)

def get_transcript(url):
    try:
        # First try YouTube captions
        video_id = extract_video_id(url)
        transcript = YouTubeTranscriptApi.get_transcript(video_id)
        return " ".join([entry['text'] for entry in transcript])
    except:
        # Fallback to audio transcription
        try:
            audio_path = download_audio(url)
            return transcribe_audio(audio_path)
        except Exception as e:
            raise ValueError(f"Both methods failed: {str(e)}")

def download_audio(url):
    try:
        yt = YouTube(url)
        audio_stream = yt.streams.filter(only_audio=True).first()
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
        audio_stream.download(filename=temp_file.name)
        return temp_file.name
    except Exception as e:
        raise ValueError(f"Audio download failed: {str(e)}")

def transcribe_audio(audio_path):
    try:
        model = whisper.load_model("base")  # Use 'small' or 'medium' for better accuracy
        result = model.transcribe(audio_path)
        return result["text"]
    except Exception as e:
        raise ValueError(f"Transcription failed: {str(e)}")
    finally:
        if os.path.exists(audio_path):
            os.remove(audio_path)

gradio_app.py:
import gradio as gr
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from app.backend.agent import generate_summary, answer_question, translate_summary
from app.backend.utils import *

css = """
.gradio-container {max-width: 900px!important}
footer {visibility: hidden}
@import url('https://fonts.boomla.com/bangla.css');
body {font-family: 'SolaimanLipi', sans-serif!important}
.dark .summary-box textarea {background-color: #1a1a1a!important}
.md_output {margin-top: 10px!important}
"""

LANGUAGE_MAPPING = {
    "English": "en_XX",
    "French": "fr_XX",
    "Spanish": "es_XX", 
    "German": "de_DE",
    "Chinese": "zh_CN",
    "Hindi": "hi_IN",
    "Arabic": "ar_AR",
    "Russian": "ru_RU",
    "Bengali": "bn_IN"
}

with gr.Blocks(theme=gr.themes.Soft(), css=css) as app:
    gr.Markdown("# YouTube AI Analyzer 3.0 ðŸš€")
    
    with gr.Tab("Video Analysis"):
        with gr.Row():
            url_input = gr.Textbox(label="YouTube URL", placeholder="Paste video URL here...")
            summary_output = gr.Textbox(label="Video Summary", lines=5, interactive=False)
        
        with gr.Row():
            with gr.Column(scale=2):
                mode_dropdown = gr.Dropdown(
                    choices=["Short", "Medium", "Detailed"],
                    value="Medium",
                    label="Summary Length",
                    interactive=True
                )
        with gr.Row():
            analyze_btn = gr.Button("Analyze Video", variant="primary")
            vis_output = gr.Image(label="Visual Summary (Mind Map)", show_label=True)
            
        with gr.Accordion("Translation Settings", open=True):
            with gr.Row():
                lang_dropdown = gr.Dropdown(
                    choices=list(LANGUAGE_MAPPING.keys()),
                    value="English",
                    label="Target Language",
                    interactive=True
                )
                translate_btn = gr.Button("Translate Summary", variant="secondary")
            
            translated_output = gr.Textbox(label="Translated Summary", lines=5, interactive=False)

    def analyze_video_handler(url, mode):
        if not url.startswith("https://www.youtube.com/"):
            raise gr.Error("Invalid YouTube URL format")
        
        try:
            transcript = get_transcript(url)
            return generate_summary(transcript, mode.lower())
            mindmap_path = generate_mindmap(summary)
        except Exception as e:
            return f"Analysis failed: {str(e)}"

    def translate_handler(summary, target_lang):
        if not summary:
            raise gr.Error("Generate a summary first before translating")
        
        lang_code = LANGUAGE_MAPPING.get(target_lang, "en_XX")
        try:
            return translate_summary(summary, lang_code)
        except Exception as e:
            return f"Translation failed: {str(e)}"

    # Event bindings
    analyze_btn.click(
        fn=analyze_video_handler,
        inputs=[url_input, mode_dropdown],
        outputs=[summary_output, vis_output]
    )
    
    translate_btn.click(
        fn=translate_handler,
        inputs=[summary_output, lang_dropdown],
        outputs=translated_output
    )

if __name__ == "__main__":
    app.launch(share=True)


requirements.txt:
# requirements.txt
youtube-transcript-api
gradio==4.25.0
python-dotenv
openai
langchain
transformers>=4.37.0
torch>=2.2.2
llama-cpp-python
accelerate>=0.31.0
sentencepiece
protobuf
langdetect==1.0.9
pytube==15.0.0
openai-whisper==20231117
ffmpeg-python==0.2.0
keybert>=0.7.0
sentence-transformers>=2.7.0
networkx==3.2.1
matplotlib==3.8.2
python-louvain==0.16
huggingface-hub==0.24.0
gradio_client==0.15.0
pydantic>=2.5.2
fastapi>=0.111.0


.gitignore: 
.env
venv/
__pycache__/
*.pyc
.venv



