Code file structure:
â”œâ”€â”€ __init__.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ src
    â””â”€â”€ app
        â”œâ”€â”€ backend
        â”‚Â Â  â”œâ”€â”€ agent.py
        â”‚Â Â  â”œâ”€â”€ __init__.py
        â”‚Â Â  â”œâ”€â”€ __pycache__
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ agent.cpython-312.pyc
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.cpython-312.pyc
        â”‚Â Â  â”‚Â Â  â””â”€â”€ utils.cpython-312.pyc
        â”‚Â Â  â””â”€â”€ utils.py
        â”œâ”€â”€ frontend
        â”‚Â Â  â”œâ”€â”€ gradio_app.py
        â”‚Â Â  â””â”€â”€ __init__.py
        â”œâ”€â”€ __init__.py
        â””â”€â”€ __pycache__
            â””â”€â”€ __init__.cpython-312.pyc

Here are the codes:
agent.py:
# backend/agent.py
#from openai import OpenAI
#from langchain.prompts import PromptTemplate
import os
from dotenv import load_dotenv
from transformers import pipeline
import torch

# Load environment variables
load_dotenv()

#client = OpenAI(OPENAI_API_KEY=os.getenv("OPENAI_API_KEY"))

#client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

MODEL_NAME = os.getenv("MODEL_NAME")  # or your chosen model

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype="auto", device_map="auto")

summarizer = pipeline(
    "summarization",
    model=MODEL_NAME,
    tokenizer=MODEL_NAME,
    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available
    framework="pt"
)



summary_template = """Generate concise video summary with timestamps:
{transcript}
"""

qa_template = """Answer using video transcript:
Question: {question}
Transcript: {transcript}
"""

def generate_summary(transcript):
    return summarizer(
        transcript,
        max_length=300,  # Increased from 150
        min_length=120,   # Increased from 40
        num_beams=4,      # Better quality generation
        no_repeat_ngram_size=3,  # Prevent repetition
        do_sample=False,
        truncation=True,
        clean_up_tokenization_spaces=True
    )[0]['summary_text']



qa_pipeline = pipeline(
    "question-answering",
    model="deepset/roberta-base-squad2"
)

def answer_question(transcript, question):
    return qa_pipeline(
        question=question,
        context=transcript[:1024]  # Model's max context
    )['answer']


# agent.py - Add translation functionality
from transformers import MBart50TokenizerFast, MBartForConditionalGeneration

translation_model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
translation_tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

LANGUAGE_MAPPING = {
    "French": "fr_XX",
    "Spanish": "es_XX",
    "German": "de_DE",
    "Chinese": "zh_CN",
    "Hindi": "hi_IN",
    "Bengali": "bn_IN"
}

def translate_summary(summary, target_lang):
    translation_tokenizer.src_lang = "en_XX"
    inputs = translation_tokenizer(
        summary, 
        return_tensors="pt", 
        truncation=True, 
        max_length=1024
    )
    
    generated_tokens = translation_model.generate(
        **inputs,
        forced_bos_token_id=translation_tokenizer.lang_code_to_id[target_lang]
    )
    
    return translation_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]


utils.py:
# backend/utils.py
from youtube_transcript_api import YouTubeTranscriptApi
import re

def extract_video_id(url):
    regex = r"(?:v=|\/)([0-9A-Za-z_-]{11}).*"
    match = re.search(regex, url)
    return match.group(1) if match else None

def get_transcript(url):
    video_id = extract_video_id(url)
    transcript = YouTubeTranscriptApi.get_transcript(video_id)
    return " ".join([entry['text'] for entry in transcript])

gradio_app.py:
import gradio as gr
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from app.backend.agent import generate_summary, answer_question
from app.backend.utils import get_transcript

css = """
.gradio-container {max-width: 900px!important}
footer {visibility: hidden}
@import url('https://fonts.boomla.com/bangla.css');
body {font-family: 'SolaimanLipi', sans-serif!important}
"""

LANGUAGE_MAPPING = {
    "English": "en_XX",
    "French": "fr_XX",
    "Spanish": "es_XX", 
    "German": "de_DE",
    "Chinese": "zh_CN",
    "Hindi": "hi_IN",
    "Arabic": "ar_AR",
    "Russian": "ru_RU",
    "Bengali": "bn_IN"
}

with gr.Blocks(theme=gr.themes.Soft(), css=css) as app:
    gr.Markdown("# YouTube AI Analyzer 2.0 ðŸš€")
    
    with gr.Tab("Video Analysis"):
        with gr.Row():
            url_input = gr.Textbox(label="YouTube URL", placeholder="Paste video URL here...")
            summary_output = gr.Textbox(label="Video Summary", lines=5, interactive=False)
        
        with gr.Row():
            analyze_btn = gr.Button("Analyze Video", variant="primary")
            
        with gr.Accordion("Translation Settings", open=True):  # Changed to open=True
            with gr.Row():
                lang_dropdown = gr.Dropdown(
                    choices=list(LANGUAGE_MAPPING.keys()),
                    value="English",
                    label="Target Language",
                    interactive=True  # Added interactive property
                )
                translate_btn = gr.Button("Translate Summary", variant="secondary")
            
            translated_output = gr.Textbox(label="Translated Summary", lines=5, interactive=False)

    # Move function inside Blocks context
    def analyze_video_handler(url):
        if not url.startswith("https://www.youtube.com/"):
            raise gr.Error("Invalid YouTube URL format")
        
        try:
            transcript = get_transcript(url)
            return generate_summary(transcript)
        except Exception as e:
            return f"Analysis failed: {str(e)}"

    # Connect the original button (removed duplicate)
    analyze_btn.click(
        fn=analyze_video_handler,
        inputs=url_input,
        outputs=summary_output
    )
    def translate_handler(summary, target_lang):
        if not summary:
            raise gr.Error("Generate a summary first before translating")
        
        lang_code = LANGUAGE_MAPPING.get(target_lang, "en_XX")
        try:
            from app.backend.agent import translate_summary
            return translate_summary(summary, lang_code)
        except ImportError:
            raise gr.Error("Translation module not found")
        except Exception as e:
            return f"Translation failed: {str(e)}"

    # Connect translate button AFTER handler definition
    translate_btn.click(
        fn=translate_handler,
        inputs=[summary_output, lang_dropdown],
        outputs=translated_output
    )

if __name__ == "__main__":
    app.launch()


requirements.txt:
# requirements.txt
youtube-transcript-api
gradio>=4.0
python-dotenv
openai
langchain
transformers
torch
llama-cpp-python
accelerate
sentencepiece
protobuf


.gitignore: 
.env
venv/
__pycache__/
*.pyc
.venv



